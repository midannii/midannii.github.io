---
layout: post
title:  "[NLP] n-gram란 ?  "
date:   2021-01-19
desc: " "
keywords: "DL, ML, NLP"
categories: [Deeplearning]
tags: [DL, ML, pytorch]
icon: icon-html
---

<br>

## Definition


N-gram은 문자열에서 N개의 연속된 요소를 추출하는 방법이다. 문자열의 처음부터 문자열 끝까지 한 글자씩 이동하면서 N글자를 추출한다.


예를 들어, 'Apple'라는 문자열을 문자(글자) 단위 2-gram으로 추출하면  ap, pp, pl, le가 된다.


<br>


문자열에서 뿐 아니라 단어 단위에서도 많이 쓰이는데, 아래 예시를 보면


```
예를 들어서 문장 An adorable little boy is spreading smiles이 있을 때, 각 n에 대해서 n-gram을 전부 구해보면 다음과 같습니다.

unigrams(n=1) : an, adorable, little, boy, is, spreading, smiles
bigrams(n=2) : an adorable, adorable little, little boy, boy is, is spreading, spreading smiles
trigrams(n=3) : an adorable little, adorable little boy, little boy is, boy is spreading, is spreading smiles
4-grams(n=4) : an adorable little boy, adorable little boy is, little boy is spreading, boy is spreading smiles

- 출처: https://wikidocs.net/21692
```


n-gram을 통한 언어 모델에서는 다음에 나올 단어의 예측은 오직 n-1개의 단어에만 의존한다.

그리고



```
'An adorable little boy is spreading' 다음에 나올 단어를 예측하고 싶다고 할 때, n=4라고 한 4-gram을 이용한 언어 모델을 사용한다고 합시다.
이 경우, spreading 다음에 올 단어를 예측하는 것은 n-1에 해당되는 앞의 3개의 단어만을 고려합니다.


만약 갖고있는 코퍼스에서 boy is spreading가 1,000번 등장했다고 합시다.
그리고 boy is spreading insults가 500번 등장했으며, boy is spreading smiles가 200번 등장했다고 합시다.
그렇게 되면 boy is spreading 다음에 insults가 등장할 확률은 50%이며, smiles가 등장할 확률은 20%입니다.

P(insults|boy is spreading)=0.500
P(smiles|boy is spreading)=0.200

확률적 선택에 따라 우리는 insults가 더 맞다고 판단하게 됩니다.


- 출처: https://wikidocs.net/21692
```



이러한 n-gram은 간단하게 구현할 수 있다.

```python
# 출처: http://www.locallyoptimal.com/blog/2013/01/20/elegant-n-gram-generation-in-python/
input_list = ['all', 'this', 'happened', 'more', 'or', 'less']

def find_ngrams(input_list, n):
    return zip(*[input_list[i:] for i in range(n)])
```


<br>


## Reference


- https://wikidocs.net/21692

- https://dojang.io/mod/page/view.php?id=2332


- http://www.locallyoptimal.com/blog/2013/01/20/elegant-n-gram-generation-in-python/
