---
layout: post
title:  "[ëª¨ë‘ë¥¼ ìœ„í•œ ë”¥ëŸ¬ë‹2 Pytorch] Linear Regression , GD  "
date:   2021-01-14
categories: ML
---

ğŸ³ reference: <ëª¨ë‘ë¥¼ ìœ„í•œ ë”¥ëŸ¬ë‹ 2: pytorch> Lab2,3

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

```

<br>

## 1. Data Definition

ê³µë¶€ ì‹œê°„ê³¼ ì ìˆ˜ë¥¼ modelingí•œë‹¤ê³  ê°€ì •í•˜ë©´,

training dataëŠ” (hours(`x`), points(`y`)), test dataëŠ” (hours)

```python
x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[2], [4], [6]])
```



## 2. Hypothesis

training dataì™€ ê°€ì¥ ì˜ ë§ëŠ” í•˜ë‚˜ì˜ ì§ì„ ì„ model ë¡œ ì‚¼ê¸°

![model](https://img1.daumcdn.net/thumb/R800x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F2669EA3E5790FD3317)

ì´ë•Œ WëŠ” weight, bëŠ” bias ì´ê³  wì™€ bë¥¼ í•™ìŠµí•´ì•¼ í•œë‹¤.

```python
W = torch.zeros(1, requires_grad=True)
b = torch.zeros(1, requires_grad=True)
hypothesis = x_train * W + b
```

ì²˜ìŒì— w,bëŠ” 0ìœ¼ë¡œ ì´ˆê¸°í™”í•´ì£¼ê³  `requires_grad = True`ë¥¼ í†µí•´ ì•ìœ¼ë¡œ í•™ìŠµí•  ê²ƒì´ë¼ ëª…ì‹œí•œë‹¤.


## 3. Compute Loss


![cost](https://miro.medium.com/max/1820/1*4tKZI0m5fwrNqvwVJXcDSg.tiff)

ìœ„ì™€ ê°™ì€ cost funtionì€ ì•„ë˜ì™€ ê°™ì´ í¸í•˜ê²Œ ê³„ì‚° ê°€ëŠ¥í•˜ë‹¤

```python
cost = torch.mean((hypothesis - y_train) ** 2)
```


## 4. Gradient descent

ì´ì œ `SGD`ë¥¼ ì´ìš©í•´ optimizeë¥¼ í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.

learning rate(lr)ì€ 0.01ì´ë‹¤

```python
optimizer = optim.SGD([W, b], lr=0.01) # [W,b]ê°€ í•™ìŠµí•  tensor

optimizer.zero_grad() #gradient ì´ˆê¸°í™”
cost.backward() # gradient ê³„ì‚°
optimizer.step() # ê°œì„ 
```


# Full code

```python
# ë°ì´í„°
x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[1], [2], [3]])
# ëª¨ë¸ ì´ˆê¸°í™”
W = torch.zeros(1, requires_grad=True)
b = torch.zeros(1, requires_grad=True)
# optimizer ì„¤ì •
optimizer = optim.SGD([W, b], lr=0.01)

nb_epochs = 1000
for epoch in range(nb_epochs + 1):

    # H(x) ê³„ì‚°
    hypothesis = x_train * W + b

    # cost ê³„ì‚°
    cost = torch.mean((hypothesis - y_train) ** 2)

    # costë¡œ H(x) ê°œì„ 
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()

    # 100ë²ˆë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥
    if epoch % 100 == 0:
        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(
            epoch, nb_epochs, W.item(), b.item(), cost.item()
        ))
```


 `pytorch` ì˜ model.mmìœ¼ë¡œ ì½”ë“œë¥¼ ì‘ì„±í•œë‹¤ë©´, ì•„ë˜ì™€ ê°™ë‹¤


```python
# ë°ì´í„°
x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[1], [2], [3]])
# ëª¨ë¸ ì´ˆê¸°í™”
model = LinearRegressionModel()
# optimizer ì„¤ì •
optimizer = optim.SGD(model.parameters(), lr=0.01)

nb_epochs = 1000
for epoch in range(nb_epochs + 1):

    # H(x) ê³„ì‚°
    prediction = model(x_train)

    # cost ê³„ì‚°
    cost = F.mse_loss(prediction, y_train)

    # costë¡œ H(x) ê°œì„ 
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()

    # 100ë²ˆë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥
    if epoch % 100 == 0:
        params = list(model.parameters())
        W = params[0].item()
        b = params[1].item()
        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(
            epoch, nb_epochs, W, b, cost.item()
        ))

```

<br>

## Biasê°€ ì—†ëŠ” modelì„ ì´ìš©í•˜ì—¬, GDë¥¼ ë” ìì„¸íˆ ì´í•´í•´ë³´ì

```python
x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[1], [2], [3]])
```


ìœ„ ë°ì´í„°ì—ì„œ, `y = Wx`ì˜ W = 1ì´ì–´ì•¼ ì •í™•í•œ ì˜ˆì¸¡ì´ ê°€ëŠ¥í•˜ë‹¤

ì¦‰, w = 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ì€ ëª¨ë¸ì´ë‹¤


-> but, ì´ê²ƒì„ ì–´ë–»ê²Œ í‰ê°€í• ê¹Œ? `cost(loss) function`!

![cost](https://smartstuartkim.files.wordpress.com/2018/12/cost.png)


ì´ëŠ” modelì˜ ì˜ˆì¸¡ê°’ì´ ì‹¤ì œê°’ê³¼ ì–¼ë§ˆë‚˜ ë‹¤ë¥¸ì§€ë¥¼ ë‚˜íƒ€ëƒ„ (ì •í™•í• ìˆ˜ë¡ 0ì— ìˆ˜ë ´)



<br>

ì•„ë˜ì™€ ê°™ì€ Mean Squared Errorë¥¼ ì´ìš©í•¨

![mse](https://smartstuartkim.files.wordpress.com/2018/12/costfunction.png)

```python
cost = torch.mean((hypothesis - y_train) ** 2)
```

<br>


ì´ëŸ° cost functionì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•´, (ì •í™•í•œ í•™ìŠµì„ ìœ„í•´)

ê¸°ìš¸ê¸°ê°€ ìŒìˆ˜ì¼ë•ŒëŠ” ë” ì»¤ì ¸ì•¼ í•˜ê³ , ê¸°ìš¸ê¸°ê°€ ì–‘ìˆ˜ì¼ë•Œì—ëŠ” ë” ì‘ì•„ì ¸ì•¼ í•œë‹¤

ë˜í•œ ê¸°ìš¸ê¸°ê°€ í´ìˆ˜ë¡ (ê°€íŒŒë¥¼ìˆ˜ë¡) wê°€ í¬ê²Œ ë°”ë€Œê³  ì‘ì„ìˆ˜ë¡ ì‘ê²Œ ë°”ë€ë‹¤

ì´ëŸ° ê¸°ìš¸ê¸° (= `gradient`)ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ì„  ì•„ë˜ì™€ ê°™ì´ ë¯¸ë¶„í•´ì•¼ í•œë‹¤.

![GD](static/assets/img/blog/GD.png)


ì´ë ‡ê²Œ gradientë¥¼ ì´ìš©í•´ cost functionì„ ì¤„ì´ëŠ” ê²ƒì„ `Gradient Descent`ë¼ê³  í•œë‹¤


```python
# ë°ì´í„°
x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[1], [2], [3]])
# ëª¨ë¸ ì´ˆê¸°í™”
W = torch.zeros(1)
# learning rate ì„¤ì •
lr = 0.1

nb_epochs = 10
for epoch in range(nb_epochs + 1):

    # H(x) ê³„ì‚°
    hypothesis = x_train * W

    # cost gradient ê³„ì‚°
    cost = torch.mean((hypothesis - y_train) ** 2)
    gradient = torch.sum((W * x_train - y_train) * x_train)

    print('Epoch {:4d}/{} W: {:.3f}, Cost: {:.6f}'.format(
        epoch, nb_epochs, W.item(), cost.item()
    ))

    # costë¡œ H(x) ê°œì„ 
    ## ë˜ëŠ” W -= lr * gradient
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()

```
