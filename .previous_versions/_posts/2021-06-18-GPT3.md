---
layout: post
title:  "[NLP] GPT III model "
date:   2021-06-18
categories: NLP Papers
---


이번엔 GPT III를 읽어보았다.

근데 너무 길어서 ㅜㅜ Introduction을 읽은 후에 아래와 같은 궁금점 몇개를 추려놓고, 이를 중심으로 읽었다.

```

1. 기존 GPT I과 어떻게 다르지 ?

2. 어떻게 성능 향상을 이끌어냈지 ?

```

<br>

# Introduction

점점 pre-train model의 크기는 커지고, 그에 비례하여 성능은 향상하고 있다.

GPTIII는 1750억 개의 parameter를 가지는 자기회귀 언어모델이다. 즉, 엄청 크니까 엄청 성능이 좋겠지?

GPT-3을 학습셋에 직접 포함되어 있지 않은 task에 대해 빠르게 적응할 수 있는지를 테스트하도록 20개 이상의 NLP 데이터셋에 대해 평가를 진행한다.

이때의 task에는 여러 최신 task를 포함한다. 각 task에 대해, GPT-3를 3가지 조건에서 평가한다:

- few-shot learning, 혹은 모델의 10/100개의 문맥창(context window)에 맞는 설명 또는 예시(demonstration)을 허용하는 문맥 내 학습 조건,

- one-shot learning; 딱 한 개의 예시만을 허용하는 조건,

- zero-shot learning; 어떤 예시도 허용되지 않고, 모델에 주어지는 것은 오직 자연어로 된 지시문인 조건.

<br>

# 1. 기존 GPT I과 어떻게 다르지 ?

<br>

# 2. 어떻게 성능 향상을 이끌어냈지 ?

<br>

# Conclusion

한계점으로는,

- 텍스트 생성에서는 전체적으로 품질은 높았지만 여전히 ​​문서 단위로 의미론적인 반복이 일어남

- 충분히 긴 문장에서는 일관성을 잃거나, 모순이 발생

- common sense  도메인을 잘 하지 못했다.

-  auto-regressive로 인한 양방향성 부족

<br>
