---
layout: post
title:  "[NLP] Mutual Information "
date:   2021-03-21
desc: " "
keywords: "DL, ML"
categories: [Python]
tags: [DL, ML, pytorch]
icon: icon-html
---

`Mutual Information` 이란, 두 random variable들이 얼마나 `mutual dependence 한 지`(상호의존적인지)를 measure 하는 방법이다. 수식은 아래와 같다.

![fig](https://t1.daumcdn.net/cfile/tistory/996B4B485B7777320A)

- `H(X,Y)`는 X, Y가 독립일 때 `H(X)+H(Y)`의 값을 가지므로, Mutual Information의 불확실성은 X, Y가 독립일 때보다 감소

- 즉 상호 의존성이 크다면, 값이 작게 나오게 될 것이다.

<br>

자연어처리에서, 의미, topic등을 기반으로 단어와 단어 사이의 상관관계 및 유사성을 표현할수도 있다.

아무래도 확률 개념이 담겨있어 괜히(?) 어려운데 ㅠㅠ 각 문장에서 단어가 나타날 확률을 바탕으로 식을 설정하면 된다.

(코드는 아직 시행착오중 )


<br>

python의 경우, 2개의 cluster의 MI를 구하는 모듈이  [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mutual_info_score.html)에서 아래와 구현되어 있다.

또한 clustering이 잘 되어있는지 평가하는 예시 코드도 존재한다.

```python
sklearn.metrics.mutual_info_score(labels_true, labels_pred, *, contingency=None)
```



<br>




<br>


## Reference

- https://nlp.stanford.edu/IR-book/html/htmledition/mutual-information-1.html

- https://lexically.net/downloads/version7/HTML/mutual_information.html

- https://dodonam.tistory.com/81

<br>
