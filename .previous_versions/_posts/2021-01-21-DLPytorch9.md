---
layout: post
title:  "[ëª¨ë‘ë¥¼ ìœ„í•œ ë”¥ëŸ¬ë‹2 Pytorch] Relu, Weight Initialization, Dropout, Batch Normalization "
date:   2021-01-21
categories: ML
---

ğŸ³ reference: <ëª¨ë‘ë¥¼ ìœ„í•œ ë”¥ëŸ¬ë‹ 2: pytorch> Lab 9


<br>

# Vanishing Gradient

ê¸°ì¡´ sigmoidë¥¼ activation funtionìœ¼ë¡œ ì‚¬ìš©í–ˆì—ˆëŠ”ë°,

![fig](https://miro.medium.com/max/3268/1*a04iKNbchayCAJ7-0QlesA.png)

ìœ„ ê·¸ë˜í”„ì—ì„œ í™•ì¸ê°€ëŠ¥í•˜ë“¯ ì–‘ëë‹¨ì—ì„œì˜ gradientëŠ” ì•„ì£¼ ì‘ì€ (0ì— ê°€ê¹Œìš´) ê°’ì´ë‹¤.

backpropagationì—ì„œ, ê³„ì‚°í•œ ê°’ì„ ì•ìœ¼ë¡œ ë³´ë‚¼ë•Œ gradientë¥¼ ê³±í•˜ê²Œ ë˜ëŠ”ë°

0ì— ê°€ê¹Œìš´ ê°’ì„ ê³±í•˜ê²Œ ë˜ë©´, Lossë¡œë¶€í„° ì „ë‹¬ë˜ëŠ” gradientê°€ ì†Œë©¸ë˜ê²Œ ëœë‹¤.

íŠ¹íˆ ì—¬ëŸ¬ layerê°€ ìŒ“ì—¬ìˆëŠ” Deep neural networkì˜ ê²½ìš° ë¬¸ì œê°€ ëœë‹¤.

ì´ëŸ° ê²½ìš°ë¥¼ vanishing gradientë¼ê³  í•œë‹¤.

# RELU

relu í•¨ìˆ˜ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.

![fig](https://cdn-images-1.medium.com/max/357/1*oePAhrm74RNnNEolprmTaQ.png)

ì´ ê²½ìš°, ì–‘ìˆ˜ì¼ë•Œ vanishing gradient ë¬¸ì œì—†ì´ gradientë¥¼ ê³±í•  ìˆ˜ ìˆë‹¤.

pytorchì—ì„œëŠ” ì•„ë˜ì™€ ê°™ì€ ì½”ë“œë¥¼ ì‚¬ìš©í•œë‹¤.

```python
relu = torch.nn.ReLU()

# ë˜ëŠ”
torch.nn.relu(x)
```

ê²½ìš°ì— ë”°ë¼ leaky reluë¥¼ ì‚¬ìš©í•˜ê¸°ë„ í•˜ëŠ”ë°,

![fig](https://lh3.googleusercontent.com/proxy/hjJVcdaXVoe24D-mo1Yqpu3OnHVlEldaITxsx8IspPPAEfPZD3G7eox2YLIvI1WGXUsEjILY2GhAhzvsisjAjEsKxvF3cG4EAWrY0Zryq7B9Kuqa0OlfQwohGkvPDzK_FwrWSp9axa-U8tvfF0yme5k6rNrzEwnkU8JrQLE31_e1SSLFyJhQ6yBNuTqy6a40YcISd5xnTGaq8J8CN1irz79VYHYa8QprJ_q1yu5HjlLjQrKNBywEQA7UXW4Ed5DLe8cO0L_aQVTjDfAgGmyYsmgUF0c)

ìœ„ì™€ ê°™ì´ ìŒìˆ˜ì¼ë•Œ 0 ì´ ì•„ë‹Œ ê°’ì„ ì¶œë ¥í•œë‹¤.

```python
torch.nn.leaky_relu(x, 0.01)
```

<br>

## optimizer in pytorch


pytorchì—ëŠ” ì•„ë˜ì™€ ê°™ì´ ë‹¤ì–‘í•œ optimizerê°€ êµ¬í˜„ë˜ì–´ ìˆë‹¤.

```python
torch.optim.SGD
torch.optim.Adadelta
torch.optim.Adagrad
torch.optim.Adam
torch.optim.SparseAdam
torch.optim.Adamax
torch.optim.ASGD
torch.optim.LBFGS
torch.optim.RMSprop
torch.optim.Rprop
```

![fig](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbQ934t%2FbtqASyVqeeD%2FozNDSKWvAbxiJb7VtgLkSk%2Fimg.png)

<br>


ì¨Œë“  [ì˜ˆì‹œ ì½”ë“œ](https://github.com/deeplearningzerotoall/PyTorch/blob/master/lab-09_2_mnist_nn.ipynb)ë¥¼ ë³´ë©´,

```python
# parameters
learning_rate = 0.001
training_epochs = 15
batch_size = 100

# dataset loader
data_loader = torch.utils.data.DataLoader(dataset=mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)

# nn layers
linear1 = torch.nn.Linear(784, 256, bias=True) # 784 = 28*28 (MNIST data image's shape)
linear2 = torch.nn.Linear(256, 256, bias=True)
linear3 = torch.nn.Linear(256, 10, bias=True) # output layer = 10 [0-9]
relu = torch.nn.ReLU()
# Initialization
torch.nn.init.normal_(linear1.weight)
torch.nn.init.normal_(linear2.weight)
torch.nn.init.normal_(linear3.weight)
# model
model = torch.nn.Sequential(linear1, relu, linear2, relu, linear3).to(device)
# define cost/loss & optimizer
criterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.
  ## 3ë²ˆì§¸ì—ì„œ relu ì ìš©í•˜ì§€ ì•Šì€ ì´ìœ ëŠ”, CrossEntropyLossë¥¼ êµ¬í• ê±°ê¸° ë•Œë¬¸
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) # ADAMì„ optimizerë¡œ ì‚¬ìš©

# training
total_batch = len(data_loader)
for epoch in range(training_epochs):
    avg_cost = 0

    for X, Y in data_loader:
        # reshape input image into [batch_size by 784]
        # label is not one-hot encoded
        X = X.view(-1, 28 * 28).to(device)
        Y = Y.to(device)

        optimizer.zero_grad()
        hypothesis = model(X)
        cost = criterion(hypothesis, Y)
        cost.backward()
        optimizer.step()

        avg_cost += cost / total_batch

    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))
# Test the model using test sets
with torch.no_grad():
    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)
    Y_test = mnist_test.test_labels.to(device)

    prediction = model(X_test)
    correct_prediction = torch.argmax(prediction, 1) == Y_test
    accuracy = correct_prediction.float().mean()
    print('Accuracy:', accuracy.item())

    # Get one and predict
    r = random.randint(0, len(mnist_test) - 1)
    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)
    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)

    print('Label: ', Y_single_data.item())
    single_prediction = model(X_single_data)
    print('Prediction: ', torch.argmax(single_prediction, 1).item())

```


<br>


# Weight Initialization

weight Initializationì€ ì„±ëŠ¥ í–¥ìƒì— ë§¤ìš° ì¤‘ìš”í•˜ë‹¤.

ê·¸ëŸ¬ë‚˜ 0ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ëŠ”ê±´ ì¢‹ì§€ ì•Šê³ (backpropagationì—ì„œ gradient ê³„ì‚° ë•Œë¬¸),

RBMì„ ì´ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤ëŠ” ê²°ê³¼ê°€ ë°í˜€ì¡Œë‹¤ (2006)


## RBM (Restricted Boltzmann machine)

![fig](https://blog.kakaocdn.net/dn/dhduHb/btqCQoXVgL4/cVKSYdEOVXsAqQ8G79kRq1/img.png)

RBMì€ ìœ„ì™€ ê°™ì€ êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆë‹¤.

`restricted `ì˜ ì˜ë¯¸ëŠ” layer ë‚´ë¶€ì—ì„œëŠ” no connectionì´ë¼ëŠ” ëœ». ê·¸ëŸ¬ë‚˜ ë‹¤ë¥¸ layerì—ì„œëŠ” fully connected ì´ë‹¤.

ì…ë ¥ xê°€ ë“¤ì–´ê°”ì„ë•Œ yë¥¼ ë‚´ë†“ê±°ë‚˜, yë¥¼ ë³´ê³  xë¥¼ ë³µì›í•  ìˆ˜ ìˆë‹¤. (encoding decoding ëŠë‚Œ)

<br>

ì´ëŸ¬í•œ RBMì„ ì´ìš©í•˜ì—¬ pre-trainingí•˜ì—¬ weight Initializationí•  ìˆ˜ ìˆë‹¤.

pre-training ë°©ì‹ì€ ì•„ë˜ì™€ ê°™ë‹¤.

![fig](https://blog.kakaocdn.net/dn/bxrqme/btqCRzdEVTF/SaNH36ObykKq3mkxLknoxK/img.png)

1. (a)ì—ì„œ, 2ê°œì˜ layerì— ëŒ€í•´ RBMìœ¼ë¡œ í•™ìŠµ - input xì™€ output yì— ëŒ€í•´, yë¥¼ ë„£ì—ˆì„ë•Œ x'ì„ ë³µì›í•˜ë„ë¡ í•™ìŠµ

2. (b)ì—ì„œ, 2ê°œì˜ layerìœ„ì˜ ìƒˆë¡œìš´ layerë¥¼ ìŒ“ëŠ”ë‹¤. ì´ë•Œ 1ì—ì„œ í•™ìŠµí•œ x, h1ì€ ê³ ì •ì‹œí‚¤ê³ , ìƒˆë¡œìš´ layerì— ëŒ€í•´ í•™ìŠµ

3. ë§ˆì§€ë§‰ layerê¹Œì§€ ë°˜ë³µ - í•œ layerì”© ìƒˆë¡œ ìŒ“ìœ¼ë©°  í•™ìŠµ

4. `Fine tuning`: í•™ìŠµì´ ì™„ë£Œë˜ë©´, ì „ì²´ë¥¼ ë‹¤ ìŒ“ì•„ì„œ í•™ìŠµ - input x ì— ëŒ€í•´ output yë¥¼ ì¶œë ¥í•˜ê³ , ì •ë‹µê³¼ ë¹„êµí•˜ì—¬ backpropagationì„ ì§„í–‰í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµ

<br>

ì‚¬ì‹¤ ìš”ì¦˜ì—ëŠ”  RBMì„ ì˜ ì‚¬ìš©í•˜ì§€ ì•Šê³ , Xaier, Heì™€ ê°™ì€ initializerë¥¼ ì‚¬ìš©í•œë‹¤.

ì´ë“¤ì€ layerì˜ íŠ¹ì„±ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì´ˆê¸°í™”ë¥¼ í•œë‹¤.

<br>

## Xavier / He Initialization


![fig](https://blog.kakaocdn.net/dn/d0qCHm/btqCSYjIzjy/rtLlKp3KkRcSG5bNdg5mkK/img.png)


êµ‰ì¥íˆ ê°„ë‹¨í•˜ê³  ê°•ë ¥í•œí¸,,

ì´ë•Œ `nin`ì€ input layerì˜ ê°œìˆ˜, `nout`ì€ output layerì˜ ê°œìˆ˜ì´ë‹¤.

ì½”ë“œëŠ” ì•„ë˜ì™€ ê°™ë‹¤.

```python
.
.
# nn layers
linear1 = torch.nn.Linear(784, 256, bias=True)
linear2 = torch.nn.Linear(256, 256, bias=True)
linear3 = torch.nn.Linear(256, 10, bias=True)
relu = torch.nn.ReLU()

# xavier initialization
torch.nn.init.xavier_uniform_(linear1.weight)
torch.nn.init.xavier_uniform_(linear2.weight)
torch.nn.init.xavier_uniform_(linear3.weight)
.
.
```



<br>


# Dropout

dropoutì€ overfittingì˜ í•´ê²°ì±… ì¤‘ í•˜ë‚˜ì¸ë°,

![fig](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRIzsTnn-rFHOUhZGNCy9VRCVgREhBf-vZG3A&usqp=CAU)

í•™ìŠµì„ ì§„í–‰í•˜ë©´ì„œ nodeë“¤ì„ ë¬´ì‘ìœ„ë¡œ ê»ë‹¤ ì¼°ë‹¤ í•˜ëŠ” ê²ƒì´ë‹¤.

ì¦‰, ì¼ë¶€ nodeë“¤ë§Œ ì‚¬ìš©í•˜ë©´ì„œ í•™ìŠµí•˜ëŠ” ê²ƒì´ë‹¤.

```python
dropout = torch.nn.Dropout(p=drop_prob)
.
.
model = torch.nn.Sequential(linear1, relu, dropout,
                            linear2, relu, dropout,
                            linear3, relu, dropout,
                            linear4, relu, dropout,
                            linear5).to(device)
```

ë§¤ í•™ìŠµë§ˆë‹¤ ë‹¤ë¥¸ í˜•íƒœì˜ networkë¥¼ ê°€ì§€ê³  í•™ìŠµí•˜ê¸° ë•Œë¬¸ì—, overfittingì„ ë§‰ì„ ìˆ˜ ìˆë‹¤.

ì•™ìƒë¸” íš¨ê³¼ë„ ë‚¼ ìˆ˜ ìˆë‹¤ëŠ” í‰ê°€ë„ ë°›ëŠ”ë‹¤.

trainí• ë•ŒëŠ” `model.train()`, í‰ê°€í•  ë•Œì—” `model.eval()`ì„ ì‚¬ìš©í•´ì•¼ í•œë‹¤.

<br>

# Gradient vanishing, Gradient exploding

ì•ì„œ ì–¸ê¸‰í–ˆë˜ gradient vanishingê³¼ ë°˜ëŒ€ë˜ëŠ” ê²ƒì´ gradient explodingì´ë‹¤.

ì¦‰ gradientê°€ ë„ˆë¬´ ì»¤ì„œ ë°œìƒí•˜ëŠ” ë¬¸ì œì´ë‹¤.

ì´ë¥¼ í•´ê²°í•  ìˆ˜ ìˆëŠ”ê²Œ activation funtion ë°”ê¾¸ê¸° (Relu, leaky relu.. ),

weight initialization(Xavier, He .. ), ì‘ì€ learning rate ì„¤ì •,

ê·¸ë¦¬ê³  barch Normalizationì´ë‹¤.

## Internal Covariate Shift

batch normalizationì„ ë“¤ì–´ê°€ê¸° ì „, gradientë¬¸ì œì˜ ì›ì¸ì¸ internal covariate shiftë¥¼ ë¨¼ì € ì‚´í´ë³´ì.

`covariate shift`ëŠ” training setì˜ ë¶„í¬ì™€ test setì˜ ë¶„í¬ê°€ ì°¨ì´ë¥¼ ê°–ëŠ” ê²ƒì´ë‹¤.

![fig](https://miro.medium.com/max/2592/1*3MLwY2rMHziKHPfZEkIuzA.png)

Internal Covariate ShiftëŠ” modelì˜ ê° layerë§ˆë‹¤ covariate shift ë¬¸ì œê°€ ë°œìƒí•œë‹¤ëŠ” ê²ƒì¸ë°,

ì ì  inputê³¼ outputì˜ ë¶„í¬ê°€ ë‹¬ë¼ì§€ëŠ” ê²ƒì´ë‹¹!

ì´ë¥¼ ì™„í™”í•˜ê¸° ìœ„í•´ input dataë¥¼ normalizationí•˜ì—¬ ì‚¬ìš©í–ˆëŠ”ë°, batch normalizationì€ ì´ì™€ ì¡°ê¸ˆ ë‹¤ë¥´ë‹¤.

# Batch Normalization

Batch Normalizationì„ í†µí•´ gradient ë¬¸ì œë“¤ì„ ê·¼ë³¸ì ìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ìˆê³ , ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.

batch normalizationì€ ê° Layerë§ˆë‹¤ ìƒê¸°ëŠ” internal covariate shiftë¥¼ í•´ê²°í•˜ê¸° ìœ„í•¨ì´ë‹¤.

(ë‹¹ì—°íˆ layerê°€ ê¹Šì–´ì§ˆìˆ˜ë¡ ì ì  shift ê°€ ì‹¬í•´ì§€ê² ì§€?)

ì¦‰, ê° layerë§ˆë‹¤ normalizationì„ ì§„í–‰ - ì´ë•Œ mini batchë§ˆë‹¤ normalizationì„ í•˜ëŠ” ê²ƒì´ë‹¹


![fig](https://blog.kakaocdn.net/dn/U4PdS/btqEgQFdPhS/mmAh3RQmKgUoAnjy87FZ11/img.png)


training datasetì—ì„œ ê°ê° sample mean, sample varianceë¥¼ êµ¬í•œ í›„ normalization í•˜ê³ ,

ì´í›„ r*x_hat + Bë¥¼ êµ¬í•œë‹¤. (ì´ë•Œ r, BëŠ” backpropagationìœ¼ë¡œ í•™ìŠµí•œ parameter) 

ì´ë•Œ ì“°ì¸ sample meanê³¼ sample varianceëŠ” ë”°ë¡œ ì €ì¥í•´ë‘”ë‹¤.

í•™ìŠµì´ ëë‚˜ë©´, test dataì— ëŒ€í•´ í•™ìŠµëœ meanê³¼ varianceë¥¼ ì‚¬ìš©í•´ normalizationí•œ í›„ (í•™ìŠµí•œ r,Bë¥¼ ì‚¬ìš©í•œë‹¤. )

ì´ë¥¼ í†µí•´ batchì˜ êµ¬ì„±ì´ ë‹¬ë¼ë„ ê°™ì€ outputì„ ë„ì¶œí•  ìˆ˜ ìˆë‹¤. (batch normalization ë¬¸ì œ í•´ê²° ê°€ëŠ¥ )


ì½”ë“œëŠ” ì•„ë˜ì™€ ê°™ë‹¤.

```python
.
.
# nn layers
linear1 = torch.nn.Linear(784, 32, bias=True)
linear2 = torch.nn.Linear(32, 32, bias=True)
linear3 = torch.nn.Linear(32, 10, bias=True)
relu = torch.nn.ReLU()
bn1 = torch.nn.BatchNorm1d(32)
bn2 = torch.nn.BatchNorm1d(32)

nn_linear1 = torch.nn.Linear(784, 32, bias=True)
nn_linear2 = torch.nn.Linear(32, 32, bias=True)
nn_linear3 = torch.nn.Linear(32, 10, bias=True)

# model
bn_model = torch.nn.Sequential(linear1, bn1, relu,
                            linear2, bn2, relu,
                            linear3).to(device)
nn_model = torch.nn.Sequential(nn_linear1, relu,
                               nn_linear2, relu,
                               nn_linear3).to(device)
# # Save Losses and Accuracies every epoch
# We are going to plot them later
train_losses = []
train_accs = []

valid_losses = []
valid_accs = []

train_total_batch = len(train_loader)
test_total_batch = len(test_loader)
for epoch in range(training_epochs):
    bn_model.train()  # set the model to train mode

    for X, Y in train_loader:
        # reshape input image into [batch_size by 784]
        # label is not one-hot encoded
        X = X.view(-1, 28 * 28).to(device)
        Y = Y.to(device)

        bn_optimizer.zero_grad()
        bn_prediction = bn_model(X)
        bn_loss = criterion(bn_prediction, Y)
        bn_loss.backward()
        bn_optimizer.step()

        nn_optimizer.zero_grad()
        nn_prediction = nn_model(X)
        nn_loss = criterion(nn_prediction, Y)
        nn_loss.backward()
        nn_optimizer.step()

    with torch.no_grad():
        bn_model.eval()     # set the model to evaluation mode

        # Test the model using train sets
        bn_loss, nn_loss, bn_acc, nn_acc = 0, 0, 0, 0
        for i, (X, Y) in enumerate(train_loader):
            X = X.view(-1, 28 * 28).to(device)
            Y = Y.to(device)

            bn_prediction = bn_model(X)
            bn_correct_prediction = torch.argmax(bn_prediction, 1) == Y
            bn_loss += criterion(bn_prediction, Y)
            bn_acc += bn_correct_prediction.float().mean()

            nn_prediction = nn_model(X)
            nn_correct_prediction = torch.argmax(nn_prediction, 1) == Y
            nn_loss += criterion(nn_prediction, Y)
            nn_acc += nn_correct_prediction.float().mean()

        bn_loss, nn_loss, bn_acc, nn_acc = bn_loss / train_total_batch, nn_loss / train_total_batch, bn_acc / train_total_batch, nn_acc / train_total_batch

        # Save train losses/acc
        train_losses.append([bn_loss, nn_loss])
        train_accs.append([bn_acc, nn_acc])
        print(
            '[Epoch %d-TRAIN] Batchnorm Loss(Acc): bn_loss:%.5f(bn_acc:%.2f) vs No Batchnorm Loss(Acc): nn_loss:%.5f(nn_acc:%.2f)' % (
            (epoch + 1), bn_loss.item(), bn_acc.item(), nn_loss.item(), nn_acc.item()))
        # Test the model using test sets
        bn_loss, nn_loss, bn_acc, nn_acc = 0, 0, 0, 0
        for i, (X, Y) in enumerate(test_loader):
            X = X.view(-1, 28 * 28).to(device)
            Y = Y.to(device)

            bn_prediction = bn_model(X)
            bn_correct_prediction = torch.argmax(bn_prediction, 1) == Y
            bn_loss += criterion(bn_prediction, Y)
            bn_acc += bn_correct_prediction.float().mean()

            nn_prediction = nn_model(X)
            nn_correct_prediction = torch.argmax(nn_prediction, 1) == Y
            nn_loss += criterion(nn_prediction, Y)
            nn_acc += nn_correct_prediction.float().mean()

        bn_loss, nn_loss, bn_acc, nn_acc = bn_loss / test_total_batch, nn_loss / test_total_batch, bn_acc / test_total_batch, nn_acc / test_total_batch

        # Save valid losses/acc
        valid_losses.append([bn_loss, nn_loss])
        valid_accs.append([bn_acc, nn_acc])
        print(
            '[Epoch %d-VALID] Batchnorm Loss(Acc): bn_loss:%.5f(bn_acc:%.2f) vs No Batchnorm Loss(Acc): nn_loss:%.5f(nn_acc:%.2f)' % (
                (epoch + 1), bn_loss.item(), bn_acc.item(), nn_loss.item(), nn_acc.item()))
        print()

print('Learning finished')

```

trainí• ë•ŒëŠ” `model.train()`, í‰ê°€í•  ë•Œì—” `model.eval()`ì„ ì‚¬ìš©í•´ì•¼ í•œë‹¤.

<br>
