---
layout: post
title:  "[ML/DL] ExtraTreesClassifier, XGBoost : further Ensemble model "
date:   2021-02-12
desc: " "
keywords: "DL, ML"
categories: [Deeplearning]
tags: [DL, ML, tensorflow]
icon: icon-html
---

오늘 다룰 ExtraTreeClassifier와 XGBoost는 모두 ensemble model의 변형이다.


![fig](https://t1.daumcdn.net/cfile/tistory/995D67335C46BA4114)


bagging과 boosting에 대해서는 [이전 글](https://midannii.github.io/deeplearning/2021/01/30/ML7.html)에 포스팅하였당

간단하게 요약하자면,

Boosting이란 약한 분류기를 결합하여 강한 분류기를 만드는 과정이고

Bagging이란 샘플을 여러 번 뽑아 각 모델을 학습시켜 결과를 aggregating 하는 방법이다.

<br>


# ExtraTreesClassifier

random forest model의 변종으로, forest tree 의 각 후보 feature을 무작위로 분할하는 식으로 무작위성을 증가 시킨다.

공홈 문서에도 아래와 같이 *extremely randomized tree classifier* 이라고 설명되어 있다.

```
An extremely randomized tree classifier.

Extra-trees differ from classic decision trees in the way they are built. When looking for the best split to separate the samples of a node into two groups, random splits are drawn for each of the max_features randomly selected features and the best split among those is chosen. When max_features is set 1, this amounts to building a totally random decision tree.

Warning: Extra-trees should only be used within ensemble methods.
```

parameter인 `splitter`로 각 노드에서 분할을 random하게 선택할지 best인 것을 선택할지를 고를 수 있고,

`min_samples_split`으로 내부 노드를 분할하는 데 필요한 최소 샘플 수를 선택할 수 있다.

extra tree의 base tree인 `ExtraTreeClassifier`는 `DecisionTreeClassifier`를 상속한 것이며,

`splitter=’best’`가 아니고 `splitter=’random’`인 것과 `max_features=’auto’`인 것을 제외하고는 DecisionTreeClassifier와 동일하다.

accuracy 자체는 random forest와 거의 동일하지만, feature의 weight가 비교적 고른 편이다.

<br>

```python
from sklearn.ensemble import ExtraTreesClassifier
```


[ExtraTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.ExtraTreeClassifier.html)를 Decision Tree로 이용한다.



<br>


# XGBoost

`Extreme Gradient Boosting`의 약자로, Gradient Boosting 알고리즘을 분산환경에서도 실행할 수 있도록 구현해놓은 라이브러리이다.

(gradient boosting 란 이전 예측기가 만든 residual error에 새로운 예측기를 학습시키는 것이다.)

훈련 속도가 매우 빠르며, cluster간의 parallelized / distributed가 가능하다.


<br>

```python
from xgboost.sklearn import XGBModel
```



<br>


# Discussion

현재 binary-class와 multi-class의 classification에 위 두 모델을 쓰고 있는데

binary-class에서는 두 모델의 성능이 거의 비슷한 반면 multi-class에서는 확연히 달랐다.

아마도 multiclass를 다루기 위해 `OneVsRestClassifier`를 진행하지 않았기 때문으로 보여진다.




<br>


# Reference

- https://swalloow.github.io/bagging-boosting/

- ExtraTreesClassifier

  - https://tensorflow.blog/2017/11/30/%EB%8D%94%EC%9A%B1-%EB%9E%9C%EB%8D%A4%ED%95%9C-%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8-%EC%9D%B5%EC%8A%A4%ED%8A%B8%EB%A6%BC-%EB%9E%9C%EB%8D%A4-%ED%8A%B8%EB%A6%ACextratreesclassifier/

- XGBoost

  - https://bcho.tistory.com/1354

  - https://3months.tistory.com/368

  - https://midannii.github.io/deeplearning/2021/01/30/ML7.html

  - https://www.shirin-glander.de/2018/11/ml_basics_gbm/

  - https://gabrielziegler3.medium.com/multiclass-multilabel-classification-with-xgboost-66195e4d9f2d


<br>
