---
layout: post
title:  "[모두를 위한 딥러닝2 Pytorch] Multivariable Linear regression & Loading data  "
date:   2021-01-14
categories: ML
---

🍳 reference: <모두를 위한 딥러닝 2: pytorch> Lab4

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
```

<br>

## Multivariable Linear regression

복수의 data를 바탕으로 하나의 값을 추론


![model](https://render.githubusercontent.com/render/math?math=cost%28W%2C%20b%29%20%3D%20%5Cfrac%7B1%7D%7Bm%7D%20%5Csum%5Em_%7Bi%3D1%7D%20%5Cleft%28%20H%28x%5E%7B%28i%29%7D%29%20-%20y%5E%7B%28i%29%7D%20%5Cright%29%5E2&mode=display)


```python
# 데이터
x_train = torch.FloatTensor([[73, 80, 75],
                             [93, 88, 93],
                             [89, 91, 90],
                             [96, 98, 100],
                             [73, 66, 70]])
y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])
```


이때, 계산 변수가 많아질 경우 벡터의 곱을 이용하는 것 처럼

pytorch에서도 `matmul()`을 이용한다

```python
hypothesis = x_train.matmul(W) + b # or .mm or @
```


cost function의 계산은 Simple Linear Regression에서와 같이 MSE를 사용한다.


```python
# cost 계산
cost = torch.mean((hypothesis - y_train) ** 2)
# optimize
optimizer.zero_grad()
cost.backward()
optimizer.step()

```


즉, x_train과 W외에는 simple linear regression과 같다




<br>


아래와 같이 `mm.Module`을 이용하면 확장성에 더 편리하다



```python
# 데이터
x_train = torch.FloatTensor([[73, 80, 75],
                             [93, 88, 93],
                             [89, 91, 90],
                             [96, 98, 100],
                             [73, 66, 70]])
y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])
# 모델 초기화
model = MultivariateLinearRegressionModel()
# optimizer 설정
optimizer = optim.SGD(model.parameters(), lr=1e-5)

nb_epochs = 20
for epoch in range(nb_epochs+1):

    # H(x) 계산
    prediction = model(x_train)

    # cost 계산
    cost = F.mse_loss(prediction, y_train)

    # cost로 H(x) 개선
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()

    # 20번마다 로그 출력
    print('Epoch {:4d}/{} Cost: {:.6f}'.format(
        epoch, nb_epochs, cost.item()
    ))
```

<br>

대부분 엄청난 양의 데이터를 사용하여 학습하는 경우가 많음

이렇게 많은 양의 data가 있을때, 원활하게 학습하는 방법 `miniBatch Gradient Descent`


이론은 [이미 정리](https://midannii.github.io/deeplearning/2021/01/12/ML3and4.html)해뒀으니, 아래와 같이 pytorch code만 정리할 것이다.

```python
from torch.utils.data import DataLoader

dataloader = DataLoader(
  dataset, batch_size = 1, shuffle = True
)

nb_epochs = 20

for epoch in range(nb_epochs+1):
  for batch_idx, samples in enumerate(dataloader):
    x_train, y_train = samples

    # H(x) 계산
    prediction = model(x_train)

    # cost 계산
    cost = F.mse_loss(prediction, y_train)

    # cost로 H(x) 개선
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()

    print('Epoch {:4d}/{} Batch{}/{} Cost: {:.6f}'.format(
        epoch, nb_epochs, batch_idx+1, len(dataloader)
    ))
```
