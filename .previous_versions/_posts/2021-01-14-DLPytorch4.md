---
layout: post
title:  "[ëª¨ë‘ë¥¼ ìœ„í•œ ë”¥ëŸ¬ë‹2 Pytorch] Multivariable Linear regression & Loading data  "
date:   2021-01-14
categories: ML
---

ğŸ³ reference: <ëª¨ë‘ë¥¼ ìœ„í•œ ë”¥ëŸ¬ë‹ 2: pytorch> Lab4

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
```

<br>

## Multivariable Linear regression

ë³µìˆ˜ì˜ dataë¥¼ ë°”íƒ•ìœ¼ë¡œ í•˜ë‚˜ì˜ ê°’ì„ ì¶”ë¡ 


![model](https://render.githubusercontent.com/render/math?math=cost%28W%2C%20b%29%20%3D%20%5Cfrac%7B1%7D%7Bm%7D%20%5Csum%5Em_%7Bi%3D1%7D%20%5Cleft%28%20H%28x%5E%7B%28i%29%7D%29%20-%20y%5E%7B%28i%29%7D%20%5Cright%29%5E2&mode=display)


```python
# ë°ì´í„°
x_train = torch.FloatTensor([[73, 80, 75],
                             [93, 88, 93],
                             [89, 91, 90],
                             [96, 98, 100],
                             [73, 66, 70]])
y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])
```


ì´ë•Œ, ê³„ì‚° ë³€ìˆ˜ê°€ ë§ì•„ì§ˆ ê²½ìš° ë²¡í„°ì˜ ê³±ì„ ì´ìš©í•˜ëŠ” ê²ƒ ì²˜ëŸ¼

pytorchì—ì„œë„ `matmul()`ì„ ì´ìš©í•œë‹¤

```python
hypothesis = x_train.matmul(W) + b # or .mm or @
```


cost functionì˜ ê³„ì‚°ì€ Simple Linear Regressionì—ì„œì™€ ê°™ì´ MSEë¥¼ ì‚¬ìš©í•œë‹¤.


```python
# cost ê³„ì‚°
cost = torch.mean((hypothesis - y_train) ** 2)
# optimize
optimizer.zero_grad()
cost.backward()
optimizer.step()

```


ì¦‰, x_trainê³¼ Wì™¸ì—ëŠ” simple linear regressionê³¼ ê°™ë‹¤




<br>


ì•„ë˜ì™€ ê°™ì´ `mm.Module`ì„ ì´ìš©í•˜ë©´ í™•ì¥ì„±ì— ë” í¸ë¦¬í•˜ë‹¤



```python
# ë°ì´í„°
x_train = torch.FloatTensor([[73, 80, 75],
                             [93, 88, 93],
                             [89, 91, 90],
                             [96, 98, 100],
                             [73, 66, 70]])
y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])
# ëª¨ë¸ ì´ˆê¸°í™”
model = MultivariateLinearRegressionModel()
# optimizer ì„¤ì •
optimizer = optim.SGD(model.parameters(), lr=1e-5)

nb_epochs = 20
for epoch in range(nb_epochs+1):

    # H(x) ê³„ì‚°
    prediction = model(x_train)

    # cost ê³„ì‚°
    cost = F.mse_loss(prediction, y_train)

    # costë¡œ H(x) ê°œì„ 
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()

    # 20ë²ˆë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥
    print('Epoch {:4d}/{} Cost: {:.6f}'.format(
        epoch, nb_epochs, cost.item()
    ))
```

<br>

ëŒ€ë¶€ë¶„ ì—„ì²­ë‚œ ì–‘ì˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•˜ëŠ” ê²½ìš°ê°€ ë§ìŒ

ì´ë ‡ê²Œ ë§ì€ ì–‘ì˜ dataê°€ ìˆì„ë•Œ, ì›í™œí•˜ê²Œ í•™ìŠµí•˜ëŠ” ë°©ë²• `miniBatch Gradient Descent`


ì´ë¡ ì€ [ì´ë¯¸ ì •ë¦¬](https://midannii.github.io/deeplearning/2021/01/12/ML3and4.html)í•´ë’€ìœ¼ë‹ˆ, ì•„ë˜ì™€ ê°™ì´ pytorch codeë§Œ ì •ë¦¬í•  ê²ƒì´ë‹¤.

```python
from torch.utils.data import DataLoader

dataloader = DataLoader(
  dataset, batch_size = 1, shuffle = True
)

nb_epochs = 20

for epoch in range(nb_epochs+1):
  for batch_idx, samples in enumerate(dataloader):
    x_train, y_train = samples

    # H(x) ê³„ì‚°
    prediction = model(x_train)

    # cost ê³„ì‚°
    cost = F.mse_loss(prediction, y_train)

    # costë¡œ H(x) ê°œì„ 
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()

    print('Epoch {:4d}/{} Batch{}/{} Cost: {:.6f}'.format(
        epoch, nb_epochs, batch_idx+1, len(dataloader)
    ))
```
