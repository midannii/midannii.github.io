---
layout: post
title:  "[Stanford/CS224n] [3] Word Window Classification, Neural Networks, and Matrix Calculus "
date:   2021-04-16
categories: CS224n 
---


<br>


오늘은 CS224n 3강 ! *Word Window Classification, Neural Networks, and Matrix Calculus* 이당

<br>

# Classification Review

## 1. classification in NLP

NLP에서, input과 output에 따라 NER(개체명을 분류), 감정분석(긍정인지 부정인지 분류) 등등 classification은 엄청 다양하댜

ML분야에서의 classification은 decision boundary를 어떻게 정하느냐에 따라 class를 정하게 되는데,

decision boundary를 결정하는 `weight`를 학습해가며 classification model을 학습한다.




<br>

### 1-1. Supervised learning



Classification의  Supervised learning에서, `input & output`의 pair로 이루어진 data set(`D = {(x1,y1), .., (xn,yn)}`)을 준비하고


이를 training , validation, test set으로 나눈 후

training dataset을 바탕으로 학습된 ML model의 output(`M(x)`)과 실제 output(`y`)을 평가할 `loss function`으로 weight를 학습시키게 된다.


```
L(M(x), y) >=0
```

이후 validation , test set으로 model을 테스트하고 성능을 평가한다.


<br>

### 1-2. Loss function

loss function에 output(`y`)을 넣으면 probability(`y'`)로 바뀐다. -> 이렇게 바뀐 확률을 확률분포(`P(y=y'|x)`)로 표현 가능


(이때 binary에서는 `logistic` function , multiclass에서는 `softmax` function 사용 )


학습된 ML model의 output(`M(x)`)이 바뀐 *조건부 확률분포*와 실제 output(`y`)가 바뀐 확률분포가 같아지도록 학습한다.


<br>

이때, 모든 tranining sample의 확률이 최대가 되는 `Maximum likelihood Estimator`를 사용한다.

```
argmax logpθ(D) = argmax Σlogpθ(yn|xn)

L(θ) = Σl(Mθ(xn), yn) = -Σlogpθ(yn|xn)

```

위처럼 log값을 갖는 식과 sigma을 이용하며, loss function(`L(θ)`)에서는 구한 log 값의 음의 합을 사용한다.

<br>

### 1-3. Cross entropy

데이터의 분포와 모델이 추정한 확률분포와 결합하여 학습하기 위해서는 cross entropy가 필요하다


![fig](https://img1.daumcdn.net/thumb/R800x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F999A69425B25187107)

위와 같이 확률이 작아질수록 값(중요도)가 커지는 entropy 식(`-logP(X)`)을 바탕으로 빈도가 적게 나타난 정보의 중요도를 높게 두므로,

여기에 정보의 등장 확률(`P(X)`)을 곱하여 평균 (`-P(X)logP(X)`)들의 합을 표현하는 엔트로피를 활용한 것이다.


```
H(P,Q) = H(P) + D(P||Q) = -Σ P(x)log(Qx)
```

위와 같은 cross entropy 식에서, 데이터의 분포 `P`와 모델이 추정한 분포(`Q`)의 사이를 최소화하기 위해, `D(P||Q)`를 최소화하게끔 한다.

<br>

# 2. Neural Network

![fig](https://i.stack.imgur.com/7mTvt.jpg)

뉴런의 구조를 응용한 neural network ! 몇가지만 적어보자면,

 activation function에 비선형 함수를 사용해야

- non-linear decision boundary를 학습할 수 있음

- layer를 쌓을 수 있음 (linear의 경우 layer를 쌓아도 `linear transform`이 됨)

  - layer가 늘어날수록 실제 data 분포와 decision boundary의 모양이 비슷해짐


![fig](https://jtsulliv.github.io/images/perceptron/linsep_new.png?raw=True)


NLP에서는, 학습에서 parameter 뿐만 아니라 `embedding된 word vector`도 함께 학습을 시킨다.

이때 `word2vec`, `Glove`, `ELMO`, `BERT` 등 다양한 word vector가 존재한다.

이때 random으로 initialize할수도 있지만, pre-train 및 fine tuning을 하기도 한다.

특히 많은 데이터로 이미 학습된 `pre-trained word vector`를 사용하므로서, 범용성뿐 아니라 정확성 향상 및 시간 절감의 효과도 있다.

(그러나 trainng dataset의 크기가 작은 경우 fine-tuning을 하지 않은 것을 추천한다. )






<br>

# 3. Window classification

대표적인 task, `NER`은 문장에서 Named Entity를 분류하는 방법이다.

이때 문맥(context)를 바탕으로 Named Entity를 결정하는데, 문맥을 읽는 방법에 따라 두가지로 나뉜다.

### 1) average

평균을 취하면 위치 정보가 사라짐


### 2) concatenate

![fig](https://user-images.githubusercontent.com/43376853/95590745-649cf900-0a81-11eb-9604-26588f5b71e7.png)


hidden layer에서 비선형 함수의 경우, relu나 sigmoid를 한번 더 사용해줘야 한다.


<br>

![fig](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fbpxuzp%2FbtquJuEus7R%2FqarSt2AS8n7hiTMT7ZUK7K%2Fimg.png)


<br>

## Max-Margin loss

![fig](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcahAE0%2FbtquJtZSzL6%2FoluSwjXi1HRzLfUQy0Luuk%2Fimg.png)

일반적으로 softmax를 사용할 경우 값을 확률 비율로 변경했기 때문에 비율간의 차이를 계산하는 Cross-entropy를 사용하는게 일반적이다.

하지만, 위에서 score함수를 직접 정했으므로 여기에서는 이에 걸맞는 max-margin loss(hinge loss)를 이용해  정답과 오답 사이의 거리를 최대로 만들어주는 margin을 찾는다.

이때 margin으로 1을 둠으로서, optimization objective가 risky해지는 것을 막아준다.

<br>

# Matrix Calculus

## gradient descent algorithm

![fig](https://i.ytimg.com/vi/b4Vyma9wPHo/maxresdefault.jpg)

이전 목적함수의 미분값(기울기)을 어느 정도로 줄 것인지를 정하는 learning rate(`α`)를 지정해준다

neural network에서, 목적함수의 미분값은 backpropagation algorithm으로 구한다.

![fig](https://i.stack.imgur.com/7Ui1C.png)


<br>

## jacobian matrix


이때 계산량이 엄청 많으니까 matrix calcus로 계산을 하면 속도가 엄청 빨라지는데,

편미분의 조합을 나타낸 행렬인 jacobian matrix를 이용하면 훨씬 빠르다!


![fig](data:image/png;base64,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)

neural network에서, 이전 layer의 n개의 node가 들어오면 현재 layer의 m개에 대해 각각 편미분한 것을 한 matrix에 나타낸 jacobian matrix를 사용하며,


이후 matrix끼리의 합성함수로 나타낼 수 있으며, chain rule로 미분한다.

이때 dimension을 맞추도록 행렬변환 및 gradient descent계산 형태 식으로 바꿔주면 잘 활용할 수 있다.

아래 코드에 해당 내용을 python으로 잘 정리해뒀더라.


[https://medium.com/unit8-machine-learning-publication/computing-the-jacobian-matrix-of-a-neural-network-in-python-4f162e5db180](https://medium.com/unit8-machine-learning-publication/computing-the-jacobian-matrix-of-a-neural-network-in-python-4f162e5db180)






# Reference

- [강의](https://www.youtube.com/watch?v=8rXD5-xhemo&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z)

- [한글 강의](https://www.youtube.com/watch?v=9woiID8QzbE&list=PLetSlH8YjIfVdobI2IkAQnNTb1Bt5Ji9U)

- [자료 ](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/)

- [참고](http://solarisailab.com/archives/959)

- [참고](https://eda-ai-lab.tistory.com/123)

<br>
