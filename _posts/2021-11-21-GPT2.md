---
layout: post
title:  [NLP] Language Models are Unsupervised Multitask Learners - GPT2
date:   2021-11-21
categories: NLP Papers
---

## * 간단 요약

- 일반적인 NLP는 특정 task에 대해 supervised learning
- 본 논문에서, WebText dataset으로 학습하면 별도의 명시적인 supervising없이 task에 대해 학습이 가능함을 보임
- document, question을 conditioning하면, 별도의 training 없이도 CoQA datset에서 F1 score 55 달성
- LM의 capacity는 , zero-shot task transfer의 성공에 필수적이며, task를 아울러 log-linear fashion의 성능 향상
    - A ***log******linear*** model is a mathematical model that takes the form of a function whose ***logarithm*** equals a ***linear*** combination of the parameters of the model
- 우리의 GPT II model은, 8개 중 7개의 ML dataset의 zero-shot setting에서 SOTA임
    - zero-shot learning이란, semantic information을 통해 unseen data(훈련 중에 관찰되지 않은 클래스의 샘플)만으로 class를 예측

---

## 1. Introduction

- 최근 ML에서는, 큰 dataset, high-capacity model, supervised learning이 결합된 system이 대세임

    → 그러나 이 system은, data나 task에 대해 not robust (즉, 특정 data나 task에만 적합한 narrow system)

    → 이러한 narrow system은, generalization이 부족함

    → 이를 극복하기 위해, 최근 GLUE와 같은 multitask benchmark dataset이 등장하는 중임

- multitask learning(이하 MTL)은 general performance를 어느정도 보장해줌
    - MTL의 generalize를 위해 데이터를 늘리지만, 데이터를 늘리는 데에는 한계가 존재

        → 따라서, 추가적인 setup을 고려해야함

- 최근 언어모델에서는 pre-training, fine-tuning의 결합으로 성능을 높히는 중임

    → 이러한 접근으로, 보다 유연한 transfer 가능 (아래와 같은 이유 때문)

    1. word vectors were learned and used as inputs to task-specific architectures
    2. the contextual representations of recurrent networks were transferred
    3. recent work suggests that task-specific architectures are no longer necessary and transferring many self-attention blocks is sufficient

    → 그러나, 이러한 방법은 여전히 supervised learning이 필요함 .. (labeling없으면, commonsense reasoning이나 감정분석같은 task만 가능)

- 본 논문에서는, 별도의 수동적인 Labeling없이도 다양한 task를 수행할 수 있는 general system을 제안

    즉, 별도의 prameter나 architecture 수정 없이도 LM이 zero-shot setting에서의 down-stream task를 잘 수행할 수 있음을 보임

- 또한 이러한 방법이, LM의 다양한 zero-shot setting에서의 잠재적인 능력을 가짐을 보임.

## 2. Approach

- single task를 학습하기 위해서는 `p(output | input)` 에 대한 conditional distribution만 잘 측정하면 됨
- 그러나, MTL을 위해서는 동일한 input에 대해, 수행해야하는 output들도 conditioning해야함 → 즉, `p(output | input, task)`
    - 언어는 task, input, output을 symbol의 sequence로서 specify하는 데에 유연한 방법들을 제공함
        - 번역에서는 (translate to french, english text, french text). 기계 독해에서는 (answer the question, document, question, answer)

            → 이런 format의 다양한 task를 infer 및 perform할 수 있음

- 또한, LM은 예측되어야 할 output의 symbol에 대해 명시적인 supervision이 없이도 원칙적으로는 MTL을 학습할 수 있음
    - supervised objective의 global minimum과 unsupervised object의 global minimum이 같음
- 이전 실험에서, 충분히 큰 LM은 간단한 setup으로 MTL을 수행할 수 있지만, supervised learning보다는 속도가 느림을 확인하였음
- 본 논문에서는 별도의 reward signal 없이 QA task를 학습시켜봄

    → 대화를 더 풍부하게 하기 위해, web text dataset 수집

    - 인터넷에는, 상호적인 의사소통에 많이 쓰이지는 않는 광대한 양의 정보가 들어있음

    → 이를 통해, "충분한 capacity를 갖는 LM은 method이 procurement에 관계없이 자연어 sequence에서의 작업을 더 잘 예측하기 위해, infer하고 perform하는 방법을 배울 것"임을 증명하고자 함

    → 만약 모델이 이를 성공한다면, unsupervised MTL을 수행하는 것임

    - 또한, 다양한 task의 zero-shot setting에서도 이를 보임

### 2.1. Training Dataset

- 다양한 domain, context에 적용할 수 있도록, 다양한 주제를 포함하는 크고 다양한 dataset을 만듦 **"WebText"**

    → web 크롤링을 통해 크고 다양한 데이터를 얻고자 함

    - 단, document quality를 위해
        - we only scraped web pages which have been curated/filtered by humans
        - we scraped all outbound links from Reddit, which received at least 3 karma
            - heuristic indicator임 ; 사람들이 흥미롭거나, 교육적이거나, 재미있다고 보았는지 여부를 알 수 있음
    - wikipedia 문서 삭제

### 2.2. Input representation

LM은 어떠한 문자열에 대해서도 확률을 계산할 수 있어야 하며, OOV 문제를 해결하기 위해 tokenization이 필수적임

- 본 논문에서의 attempt; WebText에서 byte-level LM사용 → 상당한 성능 차이 보임
    - BPE; subword 기반의 인코딩 방법으로 문자 단위로 단어를 분해하여 Vocabulary를 생성하고, 반복을 통해 빈도수가 높은 문자 쌍을 지속적으로 Vocabulary에 추가하는 방법
    - 그러나 기존의 BPE는 byte-pair가 아니라 unicode에서 작동하는 경우 많았음

        → 이를 보완하기 위해 즉 Byte 수준의 string에 BPE를 적용하고자 함

        → dog., dog?, dog!와 같이 무의미한 variation을 추가하는 것을 막기 위해, we prevent BPE from merging across character categories for any byte sequence.

- 이러한 input representation을 통해, word-level LM의 장점(empirical benefit)과 byte-level LM의 장접(generality)을 결합 가능
- 어떤 unicode string에 대해서도 확률을 할당할 수 있으므로, pre-processing, tokenization, vocab size에 무관하게 LM을 evaluate할 수 있음

### 2.3. Model

- Transformer-based architecture를 LM으로 사용
- OpenAI GPT model과 큰 틀에서 비슷, but 아래와 같은 변화 존재
    - Layer normalization이 각 sub-block의 input으로 이동함
    - 추가적인 layer normalization이 final attention block에 추가됨
    - model depth가 있는 residual path의 accumulation을 설명하는 modified initialization가 추가됨
