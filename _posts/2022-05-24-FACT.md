---
layout: post
title:  "[Few-shots/Incremental learning] Forward Compatible Few-Shot Class-Incremental Learning"
date:   2022-05-24
categories: PaperReview
---

[Forward Compatible Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2203.06953)


> abstract
>
- CVPR 2022 proceedings
- 본 논문에서는, new class instance가 불충분할때, 즉 few-shot class-incremental learning (FSCIL)를 다룸
- future update에 대해 ForwArd Compatible Training(FACT)를 제안 with “virtual prototype”

## 1. Introduction

- motivation
    - 최근 vision분야에서는 DNN으로 다양한 연구가 이루어지고 있음
    - real world에서는, training과정에서 나타나지 않은 new class가 나타나는 경우가 많음 (e.g. e-commerge에서의 새로운 type의 product)

        → 이를 위한 task가 class-incremental learning (CIL)

    - 그 중에서도, new class의 data 양이 충분하지 않은 경우 다수 존재
        - 즉, inceremental model에 few-shot image만을 학습시킴
        - data collection & labeling과정에서의 cost issue때문

        → 이러한 경우는 few-shot class-incremental learning (FSCIL)

- few-shot class-incremental learning (FSCIL)
    - old class에 대한 descriminability를 잃지 않아야 함 (do not forget !!)
    - 적은 양의 new class-instance로도 기존 model과 sequential하게 통합되어야 함
    - 일반적으로 FSCIL model은 few-shot learning의 관점에서 새로 추가된 class쪽으로 overfitting되는 경향이 있음
        - software development와 비슷하게, backward compatibility(이전 버전에서 작동하던게 새로운 버전에서도 잘 작동하는 것)이 중요

            → FSCIL에서의 backward compatibility: 이전에 학습한 old class에 대한 discriminability를 잃지 않는 것

  ![fig1](https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/2ec35ad33e444ffc21e449f1f12cc5acd8abbc1d/1-Figure1-1.png)

- 본 논문에서는,
    - embedding module을 고정시키고, new class를 incorporate함
    - software 개발에서, 이전 버전이 구리면 다음 버전도 형편없기 마련 (backward compatibility 때문)

        → 여기서 착안하여, early version에서 future extension을 미리 고려하고, 미리 대비하는 “forward compatibility” 개념 제안

    - ForwArd Compatible Training(FACT)을 제안함
        - embedding space에 multiple virtual prototype을 미리 할당(공간을 예약해둠)

            → 이러한 virtual prototype을 optimize하면서, 같은 class는 더 가까이 push하고 incoming new class에 더 많은 공간을 reserve

            - 즉, old class는 더 compact하게 만들어 new class에 대한 공간 확보
        - 이를 통해, 미래의 변화에 대한 stress를 minimize할 수 있으며, growable하고 provident한 model을 개발하고자 하였음
    - 또한, model provident를 위해 instance를 mixture하여 virtual instance를 생성함 (?)

        → virtual instance는 explicit supervision으로 embedding space를 reserve하게 함

        → 이렇게 reserved된 virtual prototype은 inference과정에서 informative한 basis vector로 작용할 수 있음

        → basis vector를 통해 model이 incremental하게 classification을 잘 수행할 수 있게 함

    - benchmark를 통해 FACT의 성능을 검증함

## 2. Related Work

- Few-Shot Learning (FSL)
    - optimizationbased methods: few-shot data로도 model이 빠르게 adaptation하게 함
    - metric-based methods: feature extraction을 위한 pretrained backbone을 utilize하고, support와 query
- Class-Incremental Learning (CIL)
    - old class의 discriminability를 잃지 않으면서 new class를 학습 시키기 위해 (new class에 대한 overfitting을 막으면서)
        - 각 parameter의 importance를 측정하고, 중요한 parameter가 바뀌는 것을 방지하거나
        - model의 discrimiability를 유지하기 위해 knowledge distillation을 수행하거나
        - overfitting을 막기 위해, old instance에 대한 rehearsal을 수행
- Few-Shot Class-Incremental Learning
    - TOPIC: using the neural gas structure to preserve the topology of features between old and new classes to resist forgetting
    - Semantic-aware knowledge distillation:  treating the word embedding as auxiliary information, and building knowledge distillation terms to resist forgetting
    - FSLL: selects a few parameters to be updated at every incremental session to resist overfitting on few-shot inputs, FSLL
    - CEC: the current state-of-the-art method, which utilizes an extra graph model to propagate context information between classifiers for adaptation
- Backward Compatible Learning
    - Forward compatibility을 통해 system은 later version의 input을 수용할 수 있으며 backward compatibility은 older legacy system과의 interoperability을 허용함
    - 최근에는 ML에 적용: model의 backward compatibility을 개선하고자 함

## 3. From Old Classes to New Classes

### 3.1. Few-Shot Class-Incremental Learning

- Base Session
    - base task $D^0$에 대한 training dataset $D^0 = {(x_i, y_i)}_{i=1}^{n_0}$를 input으로 받음(with sufficient instance)
    그리고 testing dataset  $D^0_t = {(x_i, y_i)}_{i=1}^{n_0}$으로 evaluate
        - $D^0$: base task
        - $x_i ∈ R^D$ : training instance of clas $u_i ∈ Y_0$
        - $Y_0$: base label의 label space
    - model f(x)는 emperical risk $Σ_{(x_j,y_j)∈D_t^0}l(f(x_j), y_j)$ 를 minimize하도록 학습됨
    - model f(x)는 두가지 부분으로 나뉨
        - embedding $Φ(*) = R^D → R^d$
        - linear classifier $W ∈R^{d*|Y_0|}$, $W = [w_1, ..., w_{|Y_0|}]$ 이때 $w_k$는 각 class k의 classifier
- Incremental Sessions
    - new class with insufficient instance {$D^1, ..., D^B$}가 추가적인 input으로 sequential하게 들어옴
        - 이에 대한 training dataset $D^b = {(x_i, y_i)}^{NK}_{i=1}$를 input으로 받음
            - 이때 $y_i ∈ Y_b$
            - $Y_b$: task b의 label instance 이며, b≠b’일때  $Y_b ∩ Y_{b'} =  Ø$
    - 각 dataset에서의 limited instance는 N-way K-shot format : dataset에 N개에 class가 있고, 각 class에는 k개의 example
    - model f(x)는 backward compatible을 위해 emperical risk $Σ_{(x_j,y_j)∈D_t^0∪...D^b_t}l(f(x_j), y_j)$ 를 minimize하도록 학습됨

### 3.2. Backward Compatible Training for FSCIL

- Knowledge Distillation
    - model의 backward compatible을 위해, CIL 연구에서는 cross entropy loss와 knowledge distilation loss를 사용
        - $L(x,y) = (1-λ)L_{CE}(x,y) + L_{KD}(x)$
        - $L_{KD} = Σ_{k=1}^{y_{b-1}} -S_k(\overline{W}^T\overline{Φ}(x))logS_k(W^TΦ(x))$
            - $y_{b-1}$: old class set 즉 $y_{b-1} = Y_0∪...Y_{b-1}$
            - $S_k(*)$: softmax를 거친 후 k번째 class의 확률
            - $\overline{Φ}$: $D^b$를 학습하기 전의 frozen embedding
            - $\overline{W}$:$D^b$를 학습하기 전의 frozen classifier
    - old model output과 current model의 output을 같은 scale에서 align함으로서, current model의 discriminability를 유지하게 해주고, backward compatibility에 기여함
- Prototypical Network
    - Knowledge distillation으로 인한 overfitting with few-shot input issue를 해결하고 backward compatibility를 encourage하기 위해 Few-Shot Learning algorithms 사용
    - ProtoNet에서는, cross-entropy loss로 model을 base class학습시킴
        - 이후 embedding Φ는 fix되고, 각 class의 average embedding $p_i = (1/K)Σ,_{j=1}^{D^b}I(y_i = i)Φ(x_j)$  를 추출하도록 함
        (이때 I는 indicator function)
            - 이렇게 old class의 embedding을 fix함으로서, embedding이 shifting away하는 것을 막음 → backward compatibility에 기여
        - 이러한 averaged embedding represent는 각 class의 common pattern이므로, classifier로 쓸 수 있음
            - 즉, $w_i = p_i$
- Ignorance of Forward Compatibility
    - 앞에 두 방법에서는 forward compatibility 즉 current session에서의 model training은 고려하지 않았음
        - backward compatibility만 고려하는 retrospective standpoint

    → forward compatibility를 고려하기 위한 prospective view가 필요함

    - future  possible update에 대한 대비: embedding space를 남겨두고, 가능한 패턴을 예측하고, 이후의 adaptation cost를 release (?)

## 4. Forward Compatible Training for FSCIL

- FSCIL에서 forward compatibility를 enhance하고자 함
- training target: look ahead and prepare for incoming classes in the base session

    → 2가지 측면에서 realize

    1) model이 growable하기 위해, bimodal distribution으로 posterior probability를 optimize하도록 함 `[4.1.1]`

    → 이를 위해, 각 instance에 ground-truth class외에 추가적인 class를 assign함. 이렇게 assign된 extra label은 new class를 위한 space를 reserve하기 위해 사용하며, new class는 training과정에서 explicitly하게 optimize됨

    2) model이 growable하기 위해, instance mixture를 통해 new class의 가능한 distribution을 예견하고자 하였음 `[4.1.2]`

    →이렇게 mimic된 new instance를 통해 static training을 incremental training으로 변환하고자 하였으며, training과정에서 forward compatibility이 유지됨


### 4.1. Pretrain with Virtual Prototypes

![fig2](https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/2ec35ad33e444ffc21e449f1f12cc5acd8abbc1d/4-Figure2-1.png)

**4.1.1 Allocating Virtual Prototypes**

- base session에서는 $|Y_0|$ class에 대해서만 학습하면 되지만, final session에서는 $|Y_0|+NB$ class를 다룰 수 있어야 함
    - 이에 따라 traditional training paradigm에서는 old class의 embedding을 squeeze하여 new class를 위한 공간을 마련함

        → 그러나, 많은 round에서 진행되고, FSCIL에 적합하지 않은 방법임

    - 따라서, 본 논문에서는 instance embedding과 class prototpye(=classifier weight)의 similarity를 이용
        - $p(y|x)∝sim<w_y, Φ(x)>$
            - the more similar they are, the more probably x belongs to class y
        - cosine similarity를 사용: $f(x) = (W/||W||_2)^T(Φ(x)/||Φ(x)||_2)$
- forward compatibility를 위해, virtual prototpye $P_v$를 미리 assign하고, 이를 virtual class로 treat함
    - $P_v  = [p_1, ..., p_V]∈R^{d*V}$
    - V: virtual class의 개수
    - current model의 output은 $f_v(x) = [W,P_v]^TΦ(x)$
    - current model은 V개의 virtual class를 위해 embedding space를 reserve하고자 함

        $L_v(x,y) = l(f_v(x),y)+γ l(Mask(f_v(x),y),\hat{y})$ $L_v(x,y) = l(f_v(x),y)+r l(Mask(f_v(x),y),\hat{y})$

        - $L_1$: $l(f_v(x),y)$, output과 ground-truth label을 matching하는 vanilla training loss
        - $L_2$: $l(Mask(f_v(x),y),\hat{y})$

            1) ground-truth logit을 Mask()로 mask-out 하고, 2) 나머지 부분과 pseudo label $\hat{y}$를 matching

        - $Mask(f_v(x),y) =f_v(x) ⓧ (1-OneHot(y))$
            - $\hat{y} = argmax_vp^T_vΦ(x)+|Y_0|$ : virtual class with maximum logit, acting as pseudo label
            - ⓧ : Hadamard product (element-wise multiplication)
            - 1: all-ones vector
            - $γ$: trade-off parameter
- Effect of Virtual Prototypes
    - 위 식을 통해, $f_v(x)$가 bimodal이 되도록 해줌 (Figure 2의 왼쪽)
        - $L_1$: instance가 ground-truthe claster와 가장 가까워지도록 함
        - $L_2$: 가장 가까운 virtual cluster와 instance를 match함
    - 위 식을 optimizing함으로서, non-target class prototype은 reserved virtual prototpye으로 밀리고, decision boundary도 같은 방향으로 밀림

        → 이에 따라, 다른 class 의 embedding이 compact해지고, virtual class의 embedding은 reserve됨

        → the model becomes growable and forward compatibility is enhanced

    - new class 의 개수 V개에 대해, V=NB를 default로 설정함

**4.1.2 Forecasting Virtual Instances**

- provident한 model을 만들기 위해서, model은 ‘future-proof’ 즉 미래를 대비하는 능력이 필요함
    - 만약 pre-train stage에서 novel pattern을 본다면, preserved space는 새로 들어오는 new class에 더 적합하게 될 것

        → 이를 위해, instance mixture로 new class를 generate하고, generated instance을 위한 embedding space를 reserve함

        - motivation: 두개의 different cluster 사이의 interpolation은 prediction confidence가 낮음

            → 본 논문에서는 2개의 instance를 manifold mixup으로 fuse하고, 이렇게 fuse된 instance를 새로운 virtual class로서 취급함

- 본 논문에서는, middle layer에서 embedding을 두 part로 분리함: $Φ(x) = g(h(x))$
    - mini-batch에서 다른 class를 갖는(즉 $y_i$≠$y_j$) instance쌍 $(x_i, x_j)$에 대해, 이 둘의 embedding을 다음과 같이 fuse하여 virtual instance로서 사용
        - $z = g[λh(x_i)+(1-λ)h(x_j)]$
            - z의 current output $f_v(x) = [W,P_v]^Tz$
        - 이때 $λ∈[0,1]$ 는 Beta distribution으로부터 sampling됨
    - virtual instance z가 embedding space를 reserve하도록 학습하기 위한 symmetric loss

        $L_f(z) = l(f_v(z),\hat{y}) + γl(Mask(f_v(z),\hat{y}),y^*)$

        - $L_3$: $l(f_v(z),\hat{y})$
        - $L_4$: $l(Mask(f_v(z),\hat{y}),y^*)$
        - $\hat{y} = argmax_vp^T_vΦ(x)+|Y_0|$ : virtual class 중 pseudo label
        - $y^* = argmax_kw_k^Tz$ : current known classes 중 pseudo label
        - $γ$: trade-off parameter
- Effect of Virtual Instances
    - virtual class를 생성함으로서, incoming new class의 가능한 distribution을 예상하고자 하였음 (figure 2에서 오른쪽)
        - $L_3$: instance z를 mix하여, virtual prototype으로 push하고, virtual class를 위한 space를 reserve함으로서 다른 class들로부터 멀어지게 함
        - $L_4$: prevent known class가 over-squeezed되는 것을 막기 위해, known class와 virtual class 사이를 절충하는, nearest known class로 mixed instance를 push함

        → 이에 따라, model은 future instance 를 mimic함으로서 provident해지고,  forward compatibility가 enhance됨

- Why Virtual Loss Enhance Compatibility
    - final loss $L = L_v+L_f$
    - Φ와 W의 gradient 분석: virtual prototype에 숨겨진 intrinsic intuition을 찾기 위해
        - $a_y$: softmax operator를 지난 후, class y의 확률
            - $L_1$는 $-loga_y$이며, Φ(x)의 negative gradient는 $−∇Φ(x)L_1 = w_y-Σ^{|Y_0|+V}_{i=1}a_iw_i$

                → embedding을 ground0truth center class 중심으로 push하고, virtual class를 포함한 다른 class에서 멀어지게 함

            - $L_1$과 비슷하게 $L_2 = −∇Φ(x)L_1 = w_{\hat{y}}-Σ^{|Y_0|+V}_{i=1}a_iw_i$
                - 이때의  logit $a_y$의 ground-truth class는 $L_2$를 optimize할때 mask-out 되므로, classificaiton 성능을 해치지 않으면서φ(x)의 embedding을 nearest virtual prototpye으로 push할 수 있음 (figure 2)
        - $L_3,L_4$도 마찬가지임!

            $g(x)$를  identify function이라고 하면, $−∇Φ(x_i)L_3 = λ(w_{\hat{y}}-Σ^{|Y_0|+V}_{i=1}a_kw_k)$ 이고 , $−∇Φ(x_j)L_3 = (1-λ)(w_{\hat{y}}-Σ^{|Y_0|+V}_{i=1}a_kw_k)$임

            → 이를 통해, 모든 non-target class prototype을 mixed component에서 밀어내도록 함 ($L_2$와 유사한 기능)

            → 즉, known class의 embedding이 더 compact하게 optimizing되어, forward compatibility가 enhance됨

- Pseudo Code
    - appendix에 pseudo code를 제공함
        - 각 mini-batch에서, $L_v$먼저 계산하고, dataset을 섞어서 $L_f$를 계산함
    - dataset에서, all possible pair를 combine하는 것이 아니라, same index이면서 다른 class인 instance만 mix
        - 이를 통해, 시간복잡도를 $O(N^2)$에서 $O(N)$으로 감소시킴

### 4.2. Incremental Inference with Virtual Prototypes

virtual prototype을 어떻게 inference에서 사용하는가?

→ virtual prototype을 embedding space에 encoding된 base로 간주하고, prediction결과에 대해 이 base의 영향을 고려함

- 새로운 incremental dataset $D_b$가 들어올 때마다, new class의 prototype을 $p_i = (1/K)Σ,_{j=1}^{D^b}I(y_i = i)Φ(x_j)$ 로 추출하여
classifier $W=[W;x_i,i∈Y_b]$ 를 확장함
    - class prototpye을 각 class의 representation으로 간주함
- 전체 확률의 법칙 (Law of total probability)에 의해,

    $p(y_i|Φ(x)) = p(w_i|Φ(x)) = Σ_{p_v ∈P_v}p(w_i|p_v,Φ(x))p(p_v|Φ(x))$

    - 이때  $p(p_v|Φ(x)) = (exp(p_v^TΦ(x))/
    (Σ_{p_v ∈P_v}exp(p_v^TΦ(x)))$
    - 즉, final prediction을 얻기 위해 모든 informative virtual prototpye을 고려함
    - pseudo label $\hat{y}$를 사용하여 class $y_i$의 Φ(x)를 embedding하는 경우, class $w_i$와 $p_v$간의 bimodal distribution를 따름

        → 즉, $p(Φ(x)|w_i,p_v)=ηN(Φ(x)|W_i,Σ)+(1-η)N(Φ(x)|p_v,Σ)$

        - 이때 Φ(x)는 $N (Φ(x)|wi, Σ)$와 $N (Φ(x)|p_v, Σ)$의 linear superposition인 Gaussian mixture distribution을 따름
- Bayes’ Theorem에 의해,

    $p(w_i|p_v,Φ(x))=(p(Φ(x)|w_i,p_v)p(w_i|p_v))/(Σ_{j=1}^{|y_b|}p(Φ(x)|w_j,p_v)p(w_j|p_v))$ 임

    - $|y_b|$: 이전에 보였던 class의 개수
    - $p(w_i|p_v)$ : class $w_i$와 $p_v$간의 similarity를 측정하며, Gaussian distribution을 따른다(즉 $p(w_i|p_v)=N(w_i|p_v,Σ)$)고 가정하면,

        → $p(w_i|p_v,Φ(x))=(p(Φ(x)|w_i,p_v)p(w_i|p_v))/(Σ_{j=1}^{|y_b|}p(Φ(x)|W_j,p_v)p(W_j|p_v)) = (Min_η(m(w_i,Φ(x)), m(p_v,Φ(x))))m(w_i,p_v))/(Σ_{j=1}^{|y_b|}Min_η(m(w_j,Φ(x)), m(p_v,Φ(x))))m(w_j,p_v)$

        - $m(u,t) = exp((Σ^{-1}u)^Tt-0.5u^TΣ^{-1}u)$
        - $Mix_η(a,b) = ηa+(1-η)b$
        - φ(x), p,w 는 모두 normalized
        - Σ=I

    → 따라서 위 식은,

    $(Min_η(exp(w_i^T(Φ(x)+p_v)),exp(p_v^T(Φ(x)+w_i))))/
    (Σ_{j=1}^{|y_b|}Min_η(exp(w_j^T(Φ(x)+p_v)),exp(p_v^T(Φ(x)+w_j)))$

    - η=1이면, $(exp(w_i^T(Φ(x)+p_v)))/(Σ_{j=1}^{|y_b|}exp(w_j^T(Φ(x)+p_v)))$ 즉 ProtoNet과 같음 : $p_i = (1/K)Σ,_{j=1}^{D^b}I(y_i = i)Φ(x_j)$
    - 본 논문에서는, 구현의 단순화를 위해 η=0.5로 디폴트값 설정
- 즉, pre-training에서 학습한 virtual prototype을=의 도움으로, Φ(x)의 informative한 distribution을 얻을 수 있음

    → 이는 incremental stage에서 더 좋은 incremental classifier를 구축하는 데 도움이 됨

    - 이러한 inference process는 FACT의 forward compatibility를 검증함

## 5. Experiments

FSCIL benchmark & ImageNet에서 FACT와 다른 모델을 비교하는 실험을 진행

### 5.1. Implementation Details

- Dataset


    | name | #class(#image) | split  | etc |
    | --- | --- | --- | --- |
    | CIFAR100 | 100 classes(60,000 images) | - 100 classes → 60 base classes and 40 new classes.
    - new classes : eight 5-way 5-shot incremental tasks |  |
    | CUB200-2011 | 200 classes | - 200 classes →100 base classes and 100 incremental classes
    - new classes : ten 10-way 5-shot incremental tasks | fine-grained image classification task |
    | miniImageNet | 100 classes |  | ImageNet의 subset |
    | ImageNet1000 | 1000 classes | - 600 base classes
    - the other 400 classes are formulated into eight 50-way 5-shot tasks. |  |
    | ImageNet100 | 100 classes | “ | ImageNet 1000의 subset |
- Compared methods
    - classical CIL method: iCaRL, EEIL, Pre-Allocated RPC, Rebalancing
    - current SOTA FSCIL: TOPIC, SPPR, Decoupled-DeepEMD/Cosine/NegCosine, CEC
- Training details [생략]
- Evaluation protocol
    - $A_i$ : i번째 session후 Top-1 accuracy
    - PD(dropping rate) = $A_0 - A_B$ = (base session의 accuracy) - (last session 이후의 accuracy)

### 5.2. Benchmark Comparison

- FACT가 다양한  dataset에서 모두 SOTA달성
    - CIFAT100, CUB200, miniImageNet에서 FACT가 모두 sota달성

        ![fig3](https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/2ec35ad33e444ffc21e449f1f12cc5acd8abbc1d/6-Table1-1.png)

        - CUB200 dataset에서의 자세한 성능 (table 1)
            - PD가 낮아지는 정도가 제일 적음 : 즉 forward compatibility를 위해 계속 forgetting에 resist함
    - large-scale dataset인 ImageNet100, ImageNet 1000에서 FACT가 모두 sota달성

        ![fig4](statics/assets/FACT/fig4.png)

    - forward compatibility을 고려하는 Decoupled-NegCosine보다 성능이 우수
- 기존 CIL method는 poor performance를 보임
    - 즉, output restriction을 통해 backward compatibility를 해결하고자 하는 것은 FSCIL에 적절하지 않음

### 5.3. Ablation Study

![fig5](statics/assets/FACT/fig5.png)

- forward compatible의 effectiveness를 검증하고자 함

    → FACT의 네가지 loss에 대한 abliation study진행

    - L1
        - 미래의 가능한 새로운 클래스를 고려하지 않고, 알려진 클래스로 embedding space을 overspread함.

            → forward compatibility이 부족, 제대로 작동하지 x

    - L1+L2
        - 모델은 virtual loss에 의해 새 클래스를 위한 embedding space을 reserve하는 extra virtual prototypes을 pre-assign함

            → 모델이 growable해짐, forward compatibility 향상

    - L1+L2+L3
        - pretraining process 동안 가능한 새 클래스를 예측하고 해당 space을 reserve함

            → 모델은 더 나은compatibility을 갖게 됨

    - L1+L2+L3+L4
        - final loss가 대칭이 됨

        → 즉, embedding을 compact하게 만들고, FSCIL을 용이하게 함

        - 이런 virtual prototype은 embedding space에서 인코딩된 bases로 활용될 수 있으며, 이는 Eq 10를 통한 추론에 도움이 됨
- η=1(즉 protone) vs η=0.5(즉 FACT)

    → virtual prototype의 효율성을 검증하고자 함


### 5.4. Visualization of Incremental Sessions

- CIFAR100 dataset의 decision boundary를 t-SNE로 나타냄

    ![fig6](statics/assets/FACT/fig6.png)

    - training 5 old classes & 5 virtual prototypes
    - colorful dots: known class
    - square: known class의 prototype
    - back square : virtual prototypes (embedding space는 gray)

    (a): forward compatible training reserves the embedding space for new classes (the gray parts) and makes the known class embedding more compact

    (b): The reserved space benefits the incremental learning process, and new classes do not need to squeeze the embedding of old ones during incremental learning process.


### 5.5. Further Analysis

![fig7](statics/assets/FACT/fig7.png)

- Performance Measure (Figure 7a)
    - new class & base class의 accuracy, CUB200에서 new& old class 사이의 조화 평균을 나타냄
        - new class learning 및 forgetting resistance을 확인하고자 함
        - 나머지 setting은 benchmark와 동일함
    - Decoupled-Cosine & CEC을 나타냄
        - FACT가 new class에서 더 나은 성능을 가짐을 추론가능
        - forward compatible training의 효과를 검증
- Hyper-Parameter (Figure 7b)

    두가지 hyper-parameter에 따른 성능 나타냄

    - trade-off parameter γ : {0, 1e−3, 1e−2, 1e−1, 1}
    - number of virtual prototypes V : {20, 30, 40, 50, 60}

    → FACT prefers small γ and large V.

    but V > 40인 경우 trivial이므로, γ = 0.01 및 V = NB를 기본값으로 설정하는 것이 좋음 (NB = # of new classes in total)

- Incremental Shot (Figure 7c)

    “N-way K-shot”에서 shot number인 K를 변경 (miniImageNet)

    - 클래스당 인스턴스가 많을수록 prototype estimation이 더 정확
    →  성능이 향상
    - K > 20의 경우 improvement trend가 trivial해짐

- Instance Mixture Method (Figure 7d)
    - manifold mixup외에 다양한 mixup(e.g. anilla mixup, CutMix)
    - final accuracy는 mixture method에 무관함
        - mixture weight λ이 Beta(α, α)로부터 sampling되기 때문
        - 본 논문에서는 manifold mixup with α = 0.5를 default로 사용

## 6. Limitations

- hypothesis of new class instances.
    - If sufficient new class instances are available, it turns into CIL problem

        → 이 경우, forward & backward compatibility 는 동시에 고려해야 함
