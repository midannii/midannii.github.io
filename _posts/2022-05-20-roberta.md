---
layout: post
title:  "[paper] RoBERTa: A Robustly Optimized BERT Pretraining Approach "
date:   2021-05-20
categories: PaperReview
---



### Abstract

- BERT pretraining에 대한 replication study 진행

    → key hyperparameter, training data size에 대한 영향을 분석

    → BERT가 undertrained되어 있으며, 더 나은 성능을 낼 수 있음을 알아냄

    → 이런 점들을 보안하여 RoBERTa 제안

    - Robustly optimized BERT approach

---

## 1. Introduction

- ELMO, GPT, BERT, XLNET과 같은 self-training method가 대세임
    - 그러나 computationally expensive하고, 다양한 크기의 private training data로 학습되며, modeling advance의 효과를 측정하기 어려움
- 본 논문에서는 BERT pre-training에 대한 replication study진행함

    → BERT가 undertrained되어 있으며, 더 나은 성능을 낼 수 있음을 알아냄

- 이런 점들을 보안하여 RoBERTa(Robustly optimized BERT approach)제안
    - 더 길게, 큰 배치로 학습시킴
    - next sentence prediction 을 제외함
    - 더 긴 sequence에서 학습시킴
    - dynamic masking 적용
- RoBERTa를 다양한 task에 적용하여 좋은 성능을 냄
    - 특히 CC news라는 novel dataset개발하여, downstream task에서 성능 향상을 도출함

## 2. Background

BERT의 특징 요약

- [CLS], [SEP], [EOS] token
- transformer encoder architecture
- training objective - MLM, NSP
- optimization with Adam, dropout, GELU activation
- data) BOOKCORPUS, English Wiki

## 3. Experimental Setup

![roberta](https://sooftware.io/static/26b5f2f1a6d4d031c6cf36eac285256a/03f03/roberta.webp)

### 3.1 Implementation

- 추가적으로 Adam epsilon을 적용 → 성능 향상 및 stability 개선
- full-length sequence로 학습함

### 3.2 Data

- BookCorpus, CC news, open web text, stories

### 3.3 Evaluation

- GLUE, SQuAD, RACE

## 4. Training Procedure Analysis

BERT_base model을 바탕으로 실험함

### 4.1 Static vs. Dynamic Masking

- Table 1에서, dynamic ≥ static


### 4.3 Training with large batches

- Table 3에서, batch 큰게 좋음


### 4.4 Text Encoding


- original BERT는 character-level BPE tokenizer를 사용했음
    - 아마 다른 tokenizer 쓰면 성능 좋아질 거임 → but future work로 남김

### 4.2 Model Input Format and Next Sentence Prediction

- Table 2의 결과
    - individual sentences hurt performance on downstream task
    - NSP loss를 없애는 것이 성능 향상을 가져옴
- 4가지 세팅에서 진행
    - segment-pair+NSP
    - original input format used in BERT with NSP loss
    - input = pair of segments = multiple natural sentences
    - total combined length = less than 512 tokens
- sentence-pair+NSP
    - input = pair of natural setences
    - total length = less than 512 tokens
    - batch size 크게 (SEGMENT-PAIR+NSP와 유사한 total tokens를 유지할 수 있도록)
    - retrain NSP loss
- full-sentences(remove NSP loss)
    - input = packed with full sentences
    - total length = at most 512 tokens
    - NPS loss 제거
- doc-sentences(remove NSP loss)
    - input = FULL-SENTENCES와 비슷
    - 512 tokens
    - batch size 크게 (FULL-SENTENCES와 유사한 total tokens를 유지할 수 있도록)
    - NPL loss 제거

## 5. RoBERTa

- Robustly optimized BERT approach ==RoBERTa
    - BERT_large architecture 사용
    - full-sentence without NSP loss
    - dynamic masking
    - 원래 BERT보다 10배 많은 데이터로 학습(book corpus+wikipedia+ a)

- Result

    → BERT가 undertrained되어 있으며, 더 나은 성능을 낼 수 있음을 알아냄

    → 이런 점들을 보안하여 RoBERTa 제안

- Robustly optimized BERT approach
