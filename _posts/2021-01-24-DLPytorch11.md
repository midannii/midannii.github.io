---
layout: post
title:  "[ëª¨ë‘ë¥¼ ìœ„í•œ ë”¥ëŸ¬ë‹2 Pytorch] RNN (1) "
date:   2021-01-23
desc: " "
keywords: "DL, ML"
categories: [Deeplearning]
tags: [DL, ML, pytorch]
icon: icon-html
---

ğŸ³ reference: <ëª¨ë‘ë¥¼ ìœ„í•œ ë”¥ëŸ¬ë‹ 2: pytorch> Lab11



ë¬¼ë¡  ì•„ì§ CNN ê°•ì˜ë¥¼ ë‹¤ ë“¤ì€ ê±´ ì•„ë‹ˆì§€ë§Œ ì‚¬ì •ìƒ `RNN`ì„ ë¨¼ì € ë³´ê¸°ë¡œ í–ˆë‹¤!


<br>

# RNN

RNN(Recurrent Neural Network)ì€ ì…ë ¥ê³¼ ì¶œë ¥ì„ sequence ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•˜ëŠ” ëª¨ë¸ì´ë‹¤.

ì¦‰ word, text, time series ì™€ ê°™ì´ ìˆœì„œê°€ ì¤‘ìš”í•œ(ìˆœì„œë„ dataì˜ ì¼ë¶€ê°€ ë˜ëŠ”) sequential dataì— ëŒ€í•´ ì ë‹¹í•˜ê¸°ì—, NLP ì—ì„œë„ ë§ì´ ì“°ì¸ë‹¤.



![fig](https://miro.medium.com/max/3172/1*mHimR6ok4bAEYhKESwhdrg.png)

![fig](https://i.imgur.com/s8nYcww.png)


RNNì˜ `hidden state` ë•ë¶„ì— ì „ ë‹¨ê³„ì˜ nodeì—ì„œì˜ ê²°ê³¼ê°’ì„ í˜„ ë‹¨ê³„ì˜ nodeì— ë°˜ì˜í•  ìˆ˜ ìˆê³ 


ëª¨ë“  cellì´ ì „ë¶€ parameterë¥¼ ê³µìœ í•˜ê¸°ì— sequential dataì— ì ë‹¹í•œ ê²ƒì´ë‹¤.


one-to-one ë¶€í„° many-to-manyëŠ” ì´ë¯¸ ë°°ìš´ê±°ë¼ ì¼ë‹¨ ë„˜ê¸°ê¸°!

<br>

ì§ì ‘ RNNì„ êµ¬ë™í•˜ëŠ” ì½”ë“œëŠ” ì•„ë˜ì™€ ê°™ë‹¤.

ë¬¼ë¡  character(letter) ë‹¨ìœ„ë¡œë„ ê°€ëŠ¥í•˜ì§€ë§Œ,ê¸€ì (`charseq`) ë‹¨ìœ„ë¡œ ì§„í–‰í•´ë³´ë ¤ í•œë‹¤.

ì˜ˆì œ ë¬¸ì¥ì€ 'if you want you' ì´ë‹¤.



ë¬¸ì `if you want you`ë¥¼ ëª¨ë¸ì´ ì¸ì‹í•  ìˆ˜ ìˆë„ë¡ encoding í•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•œë°,

ì´ ì½”ë“œì—ì„œëŠ” one-hot encodingì„ ì´ìš©í–ˆë‹¤.


![fig](https://miro.medium.com/max/3758/1*O_pTwOZZLYZabRjw3Ga21A.png)



<br>

ì…ë ¥ì— ë“¤ì–´ê°€ê¸° ìœ„í•´ì„œëŠ” ì•Œë§ì€ tensor manipulationì„ ê±°ì³ì•¼ í•œë‹¤.

ì´ë•Œ, pytorchì—ì„œì˜ í‘œí˜„ì€ ( _, _, _) ì´ë ‡ê²Œ ë˜ëŠ”ë°,

ì•ìë¦¬ëŠ” batch size,

ì¤‘ê°„ ìë¦¬ëŠ” letter ë˜ëŠ” charì˜ ìë¦¬ìˆ˜,

ë§ˆì§€ë§‰ ìë¦¬ëŠ” input dataì˜ ê²½ìš° inputì˜ letter ì¢…ë¥˜ (ì´ ê²½ìš° ë‹¨ì–´ì˜ ì¢…ë¥˜)ì˜ ê°œìˆ˜ ì¦‰ dimension ê°œìˆ˜ì´ê³ ,

output dataì˜ ê²½ìš° hidden sizeì™€ ë™ì¼í•˜ë‹¤.




ì˜ˆë¥¼ ë“¤ì–´ input dataê°€ 'hello' ì´ê³  hidden size = 2, batch size = 3ì¸ ê²½ìš°,

input shape = (3, 5, 4), output shape = (3, 5, 2) ì´ë‹¤.

ì•„ë˜ì˜ ê²½ìš° X.shapeì€ [1, 14, 10]ì´ê³ , y.shapeì€ [1, 14]ì´ë‹¤.




```python
import torch
import torch.optim as optim
import numpy as np

sample = "if you want you"

# make dictionary
char_set = list(set(sample))
char_dic = {c: i for i, c in enumerate(char_set)}

```

char_dicì„ ì¶œë ¥í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.


```
{' ': 8,
 'a': 3,
 'f': 0,
 'i': 7,
 'n': 1,
 'o': 6,
 't': 9,
 'u': 5,
 'w': 2,
 'y': 4}
```

ì¦‰ ê¸¸ì´ê°€ 10ê°œì¸ dictionaryê°€ ë§Œë“¤ì–´ì§„ ì…ˆì´ë‹¤.

ë”°ë¼ì„œ input data shapeì˜ ë§ˆì§€ë§‰ ìë¦¬ìˆ˜ëŠ” 10ì´ë‹¤.

ì•„ë˜ë¥¼ ë³´ë©´, ì´ ëª¨ë¸ì—ì„  hidden sizeë„ 10ìœ¼ë¡œ ì§€ì •í•œë‹¤.

ë˜í•œ encoding ê²°ê³¼, 14ê°œì˜ ê²°ê³¼ê°€ ë‚˜ì˜¤ê¸° ë•Œë¬¸ì— ë‘ë²ˆì§¸ ìë¦¬ìˆ˜ëŠ” 14ì´ë‹¤.



```python

# hyper parameters
dic_size = len(char_dic)
hidden_size = len(char_dic)
learning_rate = 0.1

sample_idx = [char_dic[c] for c in sample]
x_data = [sample_idx[:-1]]
x_one_hot = [np.eye(dic_size)[x] for x in x_data]
y_data = [sample_idx[1:]]

X = torch.FloatTensor(x_one_hot)
Y = torch.LongTensor(y_data)
```


```python

# declare RNN
rnn = torch.nn.RNN(dic_size, hidden_size, batch_first=True)

# loss & optimizer setting
criterion = torch.nn.CrossEntropyLoss()
optimizer = optim.Adam(rnn.parameters(), learning_rate)

# start training
for i in range(50):
    optimizer.zero_grad()
    outputs, _status = rnn(X)
    loss = criterion(outputs.view(-1, dic_size), Y.view(-1))
    loss.backward()
    optimizer.step()

    result = outputs.data.numpy().argmax(axis=2)
    result_str = ''.join([char_set[c] for c in np.squeeze(result)])
    print(i, "loss: ", loss.item(), "prediction: ", result, "true Y: ", y_data, "prediction str: ", result_str)
```

<br>
