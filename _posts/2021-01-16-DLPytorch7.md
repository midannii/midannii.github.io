---
layout: post
title:  "[ëª¨ë‘ë¥¼ ìœ„í•œ ë”¥ëŸ¬ë‹2 Pytorch] Tips for DL "
date:   2021-01-16
categories: ML
---

ğŸ³ reference: <ëª¨ë‘ë¥¼ ìœ„í•œ ë”¥ëŸ¬ë‹ 2: pytorch> Lab7-1

<br>

## Maximum Likelyhood Estimation(MLE)


data observationë¥¼ ê°€ì¥ ì˜ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” parameterë¥¼ ì°¾ëŠ” ê³¼ì •ì´ë‹¤.


![fig](http://zzz.bwh.harvard.edu/media/ml1.gif)


ìœ„ ê·¸ë˜í”„ì—ì„œëŠ” ë³¼ë¡í•˜ê²Œ ë†’ì€ ë¶€ë¶„ ì¦‰, p= 0.56ì¸ ì§€ì ì„ ì°¾ì•„ì•¼ í•œë‹¤.


ì´ë¥¼ ìœ„í•´ì„œëŠ” ê¸°ìš¸ê¸°ë¥¼ ì´ìš©í•˜ëŠ”ë°, ìµœëŒ€ë¥¼ ì°¾ê¸° ìœ„í•´ gradient ascentë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ìµœì†Œë¥¼ ì°¾ê¸° ìœ„í•´ gradient descentë¥¼ ì‚¬ìš©í•œë‹¤.

![fig](https://kousikk.files.wordpress.com/2014/11/screen-shot-2014-11-12-at-11-57-47-am.png)


<br>


## overfitting

ë‹¤ë§Œ, MLEëŠ” `overfitting`ì„ í”¼í•  ìˆ˜ ì—†ëŠ”ë°,

![fig](https://blog.kakaocdn.net/dn/VNGxb/btqAv1XfTgH/jhebgUTjr5SCYgGSu62NRK/img.png)

ìœ„ì™€ ê°™ì€ ê·¸ë˜í”„ë¥¼ ê·¸ë ¤ ë§Œì•½ train lossëŠ” ê°ì†Œí•˜ëŠ”ë° ë¹„í•´ validation lossëŠ” ê·¸ëŒ€ë¡œì´ê±°ë‚˜ ì¦ê°€í•œë‹¤ë©´ overfittingì„ ì˜ì‹¬ê°€ëŠ¥í•˜ë‹¤.


<br>

overfittingì„ ë§‰ê¸° ìœ„í•´

ë” ë§ì€ dataë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜, featureë¥¼ ì¤„ì´ê±°ë‚˜, regularizationì„ ì‚¬ìš©í•œë‹¤

regularizationì€ early stopping, network size ì¤„ì´ê¸°, weight í¬ê¸° ì œí•œ (decay), dropout, batch normalization ë“±ë“±ì´ ìˆë‹¤.

<br>

ì „ì²´ì ì¸ flowëŠ” ì•„ë˜ì™€ ê°™ë‹¤

```python
x_train = torch.FloatTensor([[1, 2, 1],
                             [1, 3, 2],
                             [1, 3, 4],
                             [1, 5, 5],
                             [1, 7, 5],
                             [1, 2, 5],
                             [1, 6, 6],
                             [1, 7, 7]
                            ])
y_train = torch.LongTensor([2, 2, 2, 1, 1, 1, 0, 0])
x_test = torch.FloatTensor([[2, 1, 1], [3, 1, 2], [3, 3, 4]])
y_test = torch.LongTensor([2, 2, 2])

'''
class SoftmaxClassifierModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(3, 3)
    def forward(self, x):
        return self.linear(x)

def train(model, optimizer, x_train, y_train):
    nb_epochs = 20
    for epoch in range(nb_epochs):

  # H(x) ê³„ì‚°
        prediction = model(x_train)

  # cost ê³„ì‚°
        cost = F.cross_entropy(prediction, y_train)

                # costë¡œ H(x) ê°œì„ 
        optimizer.zero_grad()
        cost.backward()
        optimizer.step()

        print('Epoch {:4d}/{} Cost: {:.6f}'.format(
                    epoch, nb_epochs, cost.item()
        ))

def test(model, optimizer, x_test, y_test):
    prediction = model(x_test)
    predicted_classes = prediction.max(1)[1]
    correct_count = (predicted_classes == y_test).sum().item()
    cost = F.cross_entropy(prediction, y_test)

    print('Accuracy: {}% Cost: {:.6f}'.format(
          correct_count / len(y_test) * 100, cost.item()
    ))
'''
model = SoftmaxClassifierModel()
optimizer = optim.SGD(model.parameters(), lr=0.1)

train(model, optimizer, x_train, y_train)
test(model, optimizer, x_test, y_test)
```

<br>

## Learning rate

learning rateê°€ ë„ˆë¬´ í¬ë©´ divergeí•˜ë©´ì„œ costê°€ ì ì  ëŠ˜ì–´ë‚ ìˆ˜ë„ ìˆë‹¤ (overshooting )

ë„ˆë¬´ ì‘ìœ¼ë©´ costê°€ ê±°ì˜ ì¤„ì–´ë“¤ì§€ ì•ŠëŠ”ë‹¤.

ë”°ë¼ì„œ ì ì ˆí•œ ìˆ«ìë¡œ ì‹œì‘í•´ ì‘ê²Œ ì¡°ì •í•˜ê³ , ë§Œì•½ costê°€ ì¤„ì–´ë“¤ì§€ ì•Šìœ¼ë©´ í¬ê²Œ ì¡°ì •í•˜ì.


<br>

## Data Preprocessing

ì „ì²˜ë¦¬ ê³¼ì •ì„ ê±°ì¹˜ë©´ ë” ì •í™•í•œ í•™ìŠµ, ì˜ˆì¸¡ ê°€ëŠ¥

e.g. zero-centerí•˜ê³  normalizeí•˜ê¸° (ì •ê·œ ë¶„í¬ë¡œ ë§Œë“¤ê¸°)

```python
x_train = torch.FloatTensor([[73, 80, 75],
                             [93, 88, 93],
                             [89, 91, 90],
                             [96, 98, 100],
                             [73, 66, 70]])
y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])

mu = x_train.mean(dim=0)
sigma = x_train.std(dim=0)
norm_x_train = (x_train - mu) / sigma
```
