---
layout: post
title:  "[모두를 위한 딥러닝2 Pytorch] Linear Regression  "
date:   2021-01-14
desc: " "
keywords: "DL, ML"
categories: [Deeplearning]
tags: [DL, ML, pytorch]
icon: icon-html
---

🍳 reference: <모두를 위한 딥러닝 2: pytorch> Lab2

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

```

<br>

## 1. Data Definition

공부 시간과 점수를 modeling한다고 가정하면,

training data는 (hours(`x`), points(`y`)), test data는 (hours)

```python
x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[2], [4], [6]])
```



## 2. Hypothesis

training data와 가장 잘 맞는 하나의 직선을 model 로 삼기

![model](https://img1.daumcdn.net/thumb/R800x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F2669EA3E5790FD3317)

이때 W는 weight, b는 bias 이고 w와 b를 학습해야 한다.

```python
W = torch.zeros(1, requires_grad=True)
b = torch.zeros(1, requires_grad=True)
hypothesis = x_train * W + b
```

처음에 w,b는 0으로 초기화해주고 `requires_grad = True`를 통해 앞으로 학습할 것이라 명시한다.


## 3. Compute Loss


![cost](https://miro.medium.com/max/1820/1*4tKZI0m5fwrNqvwVJXcDSg.tiff)

위와 같은 cost funtion은 아래와 같이 편하게 계산 가능하다

```python
cost = torch.mean((hypothesis - y_train) ** 2)
```


## 4. Gradient descent

이제 `SGD`를 이용해 optimize를 하면 아래와 같다.

learning rate(lr)은 0.01이다

```python
optimizer = optim.SGD([W, b], lr=0.01) # [W,b]가 학습할 tensor

optimizer.zero_grad() #gradient 초기화
cost.backward() # gradient 계산
optimizer.step() # 개선
```


# Full code

```python
# 데이터
x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[1], [2], [3]])
# 모델 초기화
W = torch.zeros(1, requires_grad=True)
b = torch.zeros(1, requires_grad=True)
# optimizer 설정
optimizer = optim.SGD([W, b], lr=0.01)

nb_epochs = 1000
for epoch in range(nb_epochs + 1):

    # H(x) 계산
    hypothesis = x_train * W + b

    # cost 계산
    cost = torch.mean((hypothesis - y_train) ** 2)

    # cost로 H(x) 개선
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()

    # 100번마다 로그 출력
    if epoch % 100 == 0:
        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(
            epoch, nb_epochs, W.item(), b.item(), cost.item()
        ))
```


 `pytorch` 의 model.mm으로 코드를 작성한다면, 아래와 같다


```python
# 데이터
x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[1], [2], [3]])
# 모델 초기화
model = LinearRegressionModel()
# optimizer 설정
optimizer = optim.SGD(model.parameters(), lr=0.01)

nb_epochs = 1000
for epoch in range(nb_epochs + 1):

    # H(x) 계산
    prediction = model(x_train)

    # cost 계산
    cost = F.mse_loss(prediction, y_train)

    # cost로 H(x) 개선
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()

    # 100번마다 로그 출력
    if epoch % 100 == 0:
        params = list(model.parameters())
        W = params[0].item()
        b = params[1].item()
        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(
            epoch, nb_epochs, W, b, cost.item()
        ))

```
