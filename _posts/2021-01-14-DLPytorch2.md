---
layout: post
title:  "[ëª¨ë‘ë¥¼ ìœ„í•œ ë”¥ëŸ¬ë‹2 Pytorch] Linear Regression  "
date:   2021-01-14
desc: " "
keywords: "DL, ML"
categories: [Deeplearning]
tags: [DL, ML, pytorch]
icon: icon-html
---

ğŸ³ reference: <ëª¨ë‘ë¥¼ ìœ„í•œ ë”¥ëŸ¬ë‹ 2: pytorch> Lab2

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

```

<br>

## 1. Data Definition

ê³µë¶€ ì‹œê°„ê³¼ ì ìˆ˜ë¥¼ modelingí•œë‹¤ê³  ê°€ì •í•˜ë©´,

training dataëŠ” (hours(`x`), points(`y`)), test dataëŠ” (hours)

```python
x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[2], [4], [6]])
```



## 2. Hypothesis

training dataì™€ ê°€ì¥ ì˜ ë§ëŠ” í•˜ë‚˜ì˜ ì§ì„ ì„ model ë¡œ ì‚¼ê¸°

![model](https://img1.daumcdn.net/thumb/R800x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F2669EA3E5790FD3317)

ì´ë•Œ WëŠ” weight, bëŠ” bias ì´ê³  wì™€ bë¥¼ í•™ìŠµí•´ì•¼ í•œë‹¤.

```python
W = torch.zeros(1, requires_grad=True)
b = torch.zeros(1, requires_grad=True)
hypothesis = x_train * W + b
```

ì²˜ìŒì— w,bëŠ” 0ìœ¼ë¡œ ì´ˆê¸°í™”í•´ì£¼ê³  `requires_grad = True`ë¥¼ í†µí•´ ì•ìœ¼ë¡œ í•™ìŠµí•  ê²ƒì´ë¼ ëª…ì‹œí•œë‹¤.


## 3. Compute Loss


![cost](https://miro.medium.com/max/1820/1*4tKZI0m5fwrNqvwVJXcDSg.tiff)

ìœ„ì™€ ê°™ì€ cost funtionì€ ì•„ë˜ì™€ ê°™ì´ í¸í•˜ê²Œ ê³„ì‚° ê°€ëŠ¥í•˜ë‹¤

```python
cost = torch.mean((hypothesis - y_train) ** 2)
```


## 4. Gradient descent

ì´ì œ `SGD`ë¥¼ ì´ìš©í•´ optimizeë¥¼ í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.

learning rate(lr)ì€ 0.01ì´ë‹¤

```python
optimizer = optim.SGD([W, b], lr=0.01) # [W,b]ê°€ í•™ìŠµí•  tensor

optimizer.zero_grad() #gradient ì´ˆê¸°í™”
cost.backward() # gradient ê³„ì‚°
optimizer.step() # ê°œì„ 
```


# Full code

```python
# ë°ì´í„°
x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[1], [2], [3]])
# ëª¨ë¸ ì´ˆê¸°í™”
W = torch.zeros(1, requires_grad=True)
b = torch.zeros(1, requires_grad=True)
# optimizer ì„¤ì •
optimizer = optim.SGD([W, b], lr=0.01)

nb_epochs = 1000
for epoch in range(nb_epochs + 1):

    # H(x) ê³„ì‚°
    hypothesis = x_train * W + b

    # cost ê³„ì‚°
    cost = torch.mean((hypothesis - y_train) ** 2)

    # costë¡œ H(x) ê°œì„ 
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()

    # 100ë²ˆë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥
    if epoch % 100 == 0:
        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(
            epoch, nb_epochs, W.item(), b.item(), cost.item()
        ))
```


 `pytorch` ì˜ model.mmìœ¼ë¡œ ì½”ë“œë¥¼ ì‘ì„±í•œë‹¤ë©´, ì•„ë˜ì™€ ê°™ë‹¤


```python
# ë°ì´í„°
x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[1], [2], [3]])
# ëª¨ë¸ ì´ˆê¸°í™”
model = LinearRegressionModel()
# optimizer ì„¤ì •
optimizer = optim.SGD(model.parameters(), lr=0.01)

nb_epochs = 1000
for epoch in range(nb_epochs + 1):

    # H(x) ê³„ì‚°
    prediction = model(x_train)

    # cost ê³„ì‚°
    cost = F.mse_loss(prediction, y_train)

    # costë¡œ H(x) ê°œì„ 
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()

    # 100ë²ˆë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥
    if epoch % 100 == 0:
        params = list(model.parameters())
        W = params[0].item()
        b = params[1].item()
        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(
            epoch, nb_epochs, W, b, cost.item()
        ))

```
