---
layout: post
title:  "[NLP] ELMO ? "
date:   2021-01-25
desc: " "
keywords: "DL, ML, NLP"
categories: [Deeplearning]
tags: [DL, ML, pytorch]
icon: icon-html
---

<br>

# ELMO

![fig](https://target.scene7.com/is/image/Target/GUEST_514cdbda-c80c-457e-91bb-3150361df94f?wid=488&hei=488&fmt=pjpeg)



<br>


## Definition

`elmo`는 Embeddings from Language Model라는 뜻이다!

사전 훈련된 언어 모델(Pre-trained language model)을 사용하기 때문에, eLMo라고 불린다.


### bi-LM



![fig](https://qjjnh3a9hpo1nukrg1fwoh71-wpengine.netdna-ssl.com/wp-content/uploads/2019/04/ELMo-biLSTM_web.jpg)


ELMo는 일반적인 RNN과 같은 순방향 RNN 뿐만 아니라, 반대 방향으로 문장을 스캔하는 역방향 RNN 또한 활용한다.

양쪽 방향의 언어 모델을 둘 다 활용한다고하여 `biLM(Bidirectional Language Model)`이라고 한다.

(즉 hidden layer 이 최소 2개 이상)


![fig](https://miro.medium.com/max/731/0*JK_jk7PtbKM8uYRG.PNG)



EMLO는 양방향 RNN과는 다른데,

양방향 RNN은 순방향 RNN의 hidden state 와 역방향의 RNN의 hidden state 를 다음 층의 입력으로 보내기 전에 연결(concatenate)하는 것과

biLM에서의 순방향 언어모델과 역방향 언어모델은 각각의 hidden state 만을 다음 은닉층으로 보내며 훈련시킨 후 hidden state를 연결(concatenate)시키는 것은 다르다.


<br>

## model workflow

ELMo가 임베딩 벡터(`ELMo representation`)를 얻는 과정은 아래와 같다.


1. 각 층의 출력값을 연결(concatenate)한다.

  - 이때 각 층의 출력값이란 첫번째는 임베딩 층을 말하며, 나머지 층은 각 층의 은닉 상태를 말한다.

2. 각 층의 출력 값 별로 가중치(s1, .. si)를 준다.

3. 각 층의 출력값을 모두 더한다.

4. 벡터의 크기를 결정하는 스칼라 매개변수(γ)를 곱한다.

`ELMo representation` 표현은 그 자체만으로 사용할 수 있지만, 기존의 임베딩 벡터와 함께도 사용할 수 있다.

예를 들어 text classification을 위해서 GloVe와 같은 기존의 방법론을 사용한 임베딩 벡터를 준비했다고 하면,

GloVe를 사용한 임베딩 벡터만 텍스트 분류 작업에 사용하는 것이 아니라

이렇게 준비된 `ELMo representation`을 GloVe 임베딩 벡터와 연결(concatenate)해서 입력으로 사용할 수 있다.

이 때, `ELMo representation`을 만드는데 사용되는 사전 훈련된 언어 모델의 가중치는 고정시키고, 위에서 사용한 s₁, s₂, s₃와 γ는 훈련 과정에서 학습된다.



<br>

아래 링크에서 구현된 `ELMO`를 확인할 수 있다.

- [tensorflow](https://github.com/allenai/bilm-tf)

- [pytorch](https://github.com/yongyuwen/PyTorch-Elmo-BiLSTMCRF)


<br>


# Reference


- https://wikidocs.net/33930

- https://omicro03.medium.com/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-nlp-16%EC%9D%BC%EC%B0%A8-elmo-a22ca5c287c2
