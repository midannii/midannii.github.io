---
layout: post
title:  "[NLP] GPT model ; Improving Language Understanding by Generative Pre-Training"
date:   2021-06-11
desc: " "
keywords: "DL, ML"
categories: [Deeplearning]
tags: [DL, ML]
icon: icon-html
---

`XLNET`을 읽다가, 도통 autoregressive model이 확 와닿지 않아서 먼저 GPT 읽기로 했다 !

[fig](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbnTEzO%2FbtqKmoJhrF2%2FKUWQVdaSNxPkmnm10qC10k%2Fimg.png)


GPT는 "Transformer의 Decoder를 사용하여, Seq2Seq 학습 흐름에 내부 알고리즘은 Transformer에 Decoder를 사용해 학습이 진행""되었다던데, 구체적인 flow가 궁금해서 논문을 읽어보기로 했다

논문 제목은 [Improving Language Understanding by Generative Pre-Training](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)이다.


<br>

# 1.  Introduction

- 다양한 NLU task 중, large unlabeled text corpora는 많지만, specific task를 위한 labeled data는 적음 (so, 성능향상 어려움)

  - unlabeled text corpora에 대한 Language model을 generative pre-train 후 task-specific fine-tuning하면 성능 향상 가능

  - 따라서, 본 논문에서 `semi-supervised approach`를 제안 (universal representation을 학습하기 위한 unsupervised pre-training + supervised fine-tuning)


- 아래와 같은 두가지 단계 존재

    1. unlabeled data가 NN model에서 initial parameter를 학습하도록 language model objective를 사용

    2. 1에서의 parameter를, target task를 위한 supervised objective로 fine-tuning


- Transformer, document generation, syntactic parsing을 사용

    - Long-term dependency를 다루는 데 도움을 줌

<br>

# 2. Related Work

### Semi-supervised learning for NLP

### Unsupervised pre-training

### Auxiliary training objectives

<br>

# 3. Framework

-  high-capacity language model을 학습 -> 특정 task에 대하여 라벨링된 데이터로 fine-tuning

- forward 방향으로 학습 진행

## 3.1 Unsupervised pre-training



## 3.2 Supervised fine-tuning

- LM objective에 대하여 모델을 pretraining한 후, labeld dataset 를 가지는 target task에 대해 parameter를 조정

## 3.3 Task-specific input transformations

- 각각 다른 fine-tuning task에 대해

![fig](https://vanche.github.io/assets/images/gpt/gpt_architecture.png)

<br>

# 4. Experiments

## 4.1 Setup

- BookCorpus dataset for pre-training

## 4.2 Supervised fine-tuning

![fig](https://vanche.github.io/assets/images/gpt/table1.png)

![fig](https://vanche.github.io/assets/images/gpt/table2_3.png)


<br>

# 5. Analysis

- Impact of number of layers transferred

![fig](https://vanche.github.io/assets/images/gpt/figure2.png)

## Ablation study

![fig](https://vanche.github.io/assets/images/gpt/table5.png)

<br>

# 6. Conclusion

- generative pre-training +  discriminative fine-tunning 으로 NLU task의 성능 향상

- 길고 연속적인 text를 통해 학습함으로서 world knowledge, long-range dependencies 처리 가능


<br>

# Reference

- https://vanche.github.io/NLP_Pretrained_Model_GPT/

- https://vhrehfdl.tistory.com/80

- https://ratsgo.github.io/nlpbook/docs/language_model/bert_gpt/
