---
layout: post
title:  "[NLP] GPT model "
date:   2021-06-11
desc: " "
keywords: "DL, ML"
categories: [Deeplearning]
tags: [DL, ML]
icon: icon-html
---

`XLNET`을 읽다가, 도통 autoregressive model이 확 와닿지 않아서 먼저 GPT 읽기로 했다 !

논문 제목은 [Improving Language Understanding by Generative Pre-Training](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)이다.


<br>

# 1.  Introduction


<br>

# 2. Related Work

<br>

# 3. Framework

## 3.1 Unsupervised pre-training

## 3.2 Supervised fine-tuning

## 3.3 Task-specific input transformations

<br>

# 4. Experiments

## 4.1 Setup

## 4.2 Supervised fine-tuning


<br>

# 5. Analysis


## Ablation study

<br>

# 6. Conclusion



<br>

# Reference
