---
layout: post
title:  "[ëª¨ë‘ë¥¼ ìœ„í•œ ë”¥ëŸ¬ë‹2 Pytorch] CNN with mnist  "
date:   2021-01-23
desc: " "
keywords: "DL, ML"
categories: [Deeplearning]
tags: [DL, ML, pytorch]
icon: icon-html
---

ğŸ³ reference: <ëª¨ë‘ë¥¼ ìœ„í•œ ë”¥ëŸ¬ë‹ 2: pytorch> Lab10


ì´ì œ ë³¸ê²©ì ìœ¼ë¡œ CNN ì‹œì‘!


<br>

# Convolution

![fig](https://miro.medium.com/max/1164/1*0I22V_Kaf-QowUA8IQRw5w.png)

ì´ë¯¸ì§€ ìœ„ì—ì„œ `stride` ë§Œí¼ filterë¥¼ ì´ë™ì‹œí‚¤ë©´ì„œ ê²¹ì³ì§€ëŠ” ë¶€ë¶„ì˜ ê° element ê°’ì„ ê³±í•´ì„œ ëª¨ë‘ ë”í•œ ê°’ì„ ì¶œë ¥ìœ¼ë¡œ í•˜ëŠ” ì—°ì‚°ì„ ì˜ë¯¸í•œë‹¤.


![fig](https://blog.kakaocdn.net/dn/46XO9/btqGeaVthvD/p5yHwE3zk2CPkfdL7j45r1/img.png)


`stride`ëŠ” ì–¼ë§ˆë‚˜ filterë¥¼ ì´ë™í•  ê²ƒì¸ì§€ë¥¼, `padding`ì€ ê°€ì¥ìë¦¬ ì—°ì‚°ì˜ ì†ì‹¤ì„ ë§‰ê¸° ìœ„í•´ ìƒí•˜ì¢Œìš°ì— 0ìœ¼ë¡œ ê°€ë“ì°¬ padë¥¼ ì±„ì›Œì£¼ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.

<br>


ì‚¬ì‹¤ CNNì˜ ê¸°ë³¸ ì›ë¦¬, ê° ìš©ì–´ì˜ ê°œë…(kernel, padding, pooling, ..)ë“¤ì€ [ëª¨ëª¨ë”¥1](https://www.youtube.com/watch?v=Em63mknbtWo&list=PLlMkM4tgfjnLSOjrEJN31gZATbcj_MpUm&index=35)ì—ì„œ í›¨ì”¬ ìì„¸í•˜ê²Œ ë‚˜ì™€ìˆê³ ,

ì´ë²ˆ ëª¨ëª¨ë”¥ 2ì—ì„œëŠ” pytorch ë²„ì „ ìœ„ì£¼ë¡œ ê³µë¶€í–ˆë‹¤.

```python
import pytorch

conv = torch.nn.Conv2d(input_channel, output_channel, kernal_size, ...)
output = conv(input)
```

ì´ ì—°ì‚°ì—ì„œ `input`ì˜ typeì€ torch.Tensor,  inputì˜ shape ì€ (batch size, channel, height, width) ì´ë‹¤.

![fig](https://miro.medium.com/fit/c/1796/541/1*3632mUO_Nf46Kr__ZSFpcg.png)

ì´ë•Œ output size = (input size - filter size + 2*padding)/stride + 1 ì´ë‹¤.

ë§Œì•½ input size = 32x32, filter size(=filter size) = 5x5, stride = 1, padding = 2ì´ë©´,

output size = (32-5+2*2)/1 + 1 = 31 ì´ ëœë‹¤.



```python
import torch

conv = torch.nn.Conv2d(1,1,5,stride = 1, padding = 2)
inputs = torch.Tensor(1,1,32,32)
out = conv(inputs)
print(out.shape)
# torch.Size([1, 1, 32, 32])
```



ë§Œì•½ input size = 32x64, filter size = 5x5, stride = 1, padding = 0ì´ë©´,

output size = 28x60 ì´ ëœë‹¤.



```python
import torch

conv = torch.nn.Conv2d(1,1,5,stride = 1)
inputs = torch.Tensor(1,1,32,64)
out = conv(inputs)
print(out.shape)
# torch.Size([1, 1, 28, 60])
```

<br>

** CNN ì—ì„œëŠ” Perceptronì˜ weight ê°’ìœ¼ë¡œ filterê°€ ì ìš©ëœë‹¤. **

<br>



![fig](https://blog.kakaocdn.net/dn/dzTI4u/btqznO6jsqC/Jhq5kPYymM8Lsh3y7Kvovk/img.png)


poolingë„ ì•„ë˜ì™€ ê°™ì´ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.



```python
import torch

pool = torch.nn.MaxPool2d(kernal_size, stride, padding , ...)
```



<br>


ì´ì œ ê°€ì¥ ê°„ë‹¨í•œ CNN ëª¨ë¸ì„ ë§Œë“¤ì–´ë³´ì.


```python
import torch

conv1 = torch.nn.Conv2d(1,1,2,stride = 1)
pool = torch.nn.MaxPool2d(2)

inputs = torch.Tensor(1,1,28,28)
out1 = conv1(inputs)
out2 = pool(out1)
out1.size() #torch.Size([1, 1, 27, 27])
out2.size() #torch.Size([1, 1, 13, 13])
```


<br>


MNIST ì— CNNì„ ì ìš©í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤. [click reference!](https://github.com/deeplearningzerotoall/PyTorch/blob/master/lab-10_1_mnist_cnn.ipynb)


```python

# Lab 11 MNIST and Convolutional Neural Network
import torch
import torchvision.datasets as dsets
import torchvision.transforms as transforms
import torch.nn.init

device = 'cuda' if torch.cuda.is_available() else 'cpu'

# for reproducibility
torch.manual_seed(777)
if device == 'cuda':
    torch.cuda.manual_seed_all(777)

# parameters
learning_rate = 0.001
training_epochs = 15
batch_size = 100

# MNIST dataset
mnist_train = dsets.MNIST(root='MNIST_data/',
                          train=True,
                          transform=transforms.ToTensor(),
                          download=True)

mnist_test = dsets.MNIST(root='MNIST_data/',
                         train=False,
                         transform=transforms.ToTensor(),
                         download=True)

# dataset loader
data_loader = torch.utils.data.DataLoader(dataset=mnist_train,
                                          batch_size=batch_size,
                                          shuffle=True,
                                          drop_last=True)

# CNN Model (2 conv layers)
class CNN(torch.nn.Module):

    def __init__(self):
        super(CNN, self).__init__()
        # L1 ImgIn shape=(?, 28, 28, 1)
        #    Conv     -> (?, 28, 28, 32)
        #    Pool     -> (?, 14, 14, 32)
        self.layer1 = torch.nn.Sequential(
            torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),
            torch.nn.ReLU(),
            torch.nn.MaxPool2d(kernel_size=2, stride=2))
        # L2 ImgIn shape=(?, 14, 14, 32)
        #    Conv      ->(?, 14, 14, 64)
        #    Pool      ->(?, 7, 7, 64)
        self.layer2 = torch.nn.Sequential(
            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),
            torch.nn.ReLU(),
            torch.nn.MaxPool2d(kernel_size=2, stride=2))
        # Final FC 7x7x64 inputs -> 10 outputs
        self.fc = torch.nn.Linear(7 * 7 * 64, 10, bias=True)
        torch.nn.init.xavier_uniform_(self.fc.weight)

    def forward(self, x):
        out = self.layer1(x)
        out = self.layer2(out)
        out = out.view(out.size(0), -1)   # Flatten them for FC
        out = self.fc(out)
        return out

# instantiate CNN model
model = CNN().to(device)

# define cost/loss & optimizer
criterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# train my model
total_batch = len(data_loader)
print('Learning started. It takes sometime.')
for epoch in range(training_epochs):
    avg_cost = 0

    for X, Y in data_loader:
        # image is already size of (28x28), no reshape
        # label is not one-hot encoded
        X = X.to(device)
        Y = Y.to(device)

        optimizer.zero_grad()
        hypothesis = model(X)
        cost = criterion(hypothesis, Y)
        cost.backward()
        optimizer.step()

        avg_cost += cost / total_batch

    print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_cost))

print('Learning Finished!')

# Test model and check accuracy
with torch.no_grad():
    X_test = mnist_test.test_data.view(len(mnist_test), 1, 28, 28).float().to(device)
    Y_test = mnist_test.test_labels.to(device)

    prediction = model(X_test)
    correct_prediction = torch.argmax(prediction, 1) == Y_test
    accuracy = correct_prediction.float().mean()
    print('Accuracy:', accuracy.item())

```

<br>


tensorflowì˜ ê²½ìš°ì—ëŠ” ì•„ë˜ì™€ ê°™ë‹¤. [click reference](https://github.com/deeplearningzerotoall/TensorFlow/blob/master/lab-11-2-mnist-cnn-keras-sequential-eager.ipynb)

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.utils import to_categorical
import numpy as np
import matplotlib.pyplot as plt
import os

tf.enable_eager_execution()

## Hyper Parameters
learning_rate = 0.001
training_epochs = 15
batch_size = 100

##Creating a Checkpoint Directory
cur_dir = os.getcwd()
ckpt_dir_name = 'checkpoints'
model_dir_name = 'minst_cnn_seq'

checkpoint_dir = os.path.join(cur_dir, ckpt_dir_name, model_dir_name)
os.makedirs(checkpoint_dir, exist_ok=True)

checkpoint_prefix = os.path.join(checkpoint_dir, model_dir_name)

## MNIST Dataset
mnist = keras.datasets.mnist
class_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']

## Datasets
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()    

train_images = train_images.astype(np.float32) / 255.
test_images = test_images.astype(np.float32) / 255.
train_images = np.expand_dims(train_images, axis=-1)
test_images = np.expand_dims(test_images, axis=-1)

train_labels = to_categorical(train_labels, 10)
test_labels = to_categorical(test_labels, 10)    

train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(
                buffer_size=100000).batch(batch_size)
test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(batch_size)

## Model Function
def create_model():
    model = keras.Sequential()
    model.add(keras.layers.Conv2D(filters=32, kernel_size=3, activation=tf.nn.relu, padding='SAME',
                                  input_shape=(28, 28, 1)))
    model.add(keras.layers.MaxPool2D(padding='SAME'))
    model.add(keras.layers.Conv2D(filters=64, kernel_size=3, activation=tf.nn.relu, padding='SAME'))
    model.add(keras.layers.MaxPool2D(padding='SAME'))
    model.add(keras.layers.Conv2D(filters=128, kernel_size=3, activation=tf.nn.relu, padding='SAME'))
    model.add(keras.layers.MaxPool2D(padding='SAME'))
    model.add(keras.layers.Flatten())
    model.add(keras.layers.Dense(256, activation=tf.nn.relu))
    model.add(keras.layers.Dropout(0.4))
    model.add(keras.layers.Dense(10))
    return model

model = create_model()

## Loss Function
def loss_fn(model, images, labels):
    logits = model(images, training=True)
    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(
            logits=logits, labels=labels))    
    return loss

## Calculating Gradient
def grad(model, images, labels):
    with tf.GradientTape() as tape:
        loss = loss_fn(model, images, labels)
    return tape.gradient(loss, model.variables)

## Caculating Model's Accuracy
def evaluate(model, images, labels):
    logits = model(images, training=False)
    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    return accuracy

## Optimizer
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)

## Creating a Checkpoint
checkpoint = tf.train.Checkpoint(cnn=model)

# train my model
print('Learning started. It takes sometime.')
for epoch in range(training_epochs):
    avg_loss = 0.
    avg_train_acc = 0.
    avg_test_acc = 0.
    train_step = 0
    test_step = 0

    for images, labels in train_dataset:
        grads = grad(model, images, labels)                
        optimizer.apply_gradients(zip(grads, model.variables))
        loss = loss_fn(model, images, labels)
        acc = evaluate(model, images, labels)
        avg_loss = avg_loss + loss
        avg_train_acc = avg_train_acc + acc
        train_step += 1
    avg_loss = avg_loss / train_step
    avg_train_acc = avg_train_acc / train_step

    for images, labels in test_dataset:        
        acc = evaluate(model, images, labels)        
        avg_test_acc = avg_test_acc + acc
        test_step += 1    
    avg_test_acc = avg_test_acc / test_step    

    print('Epoch:', '{}'.format(epoch + 1), 'loss =', '{:.8f}'.format(avg_loss),
          'train accuracy = ', '{:.4f}'.format(avg_train_acc),
          'test accuracy = ', '{:.4f}'.format(avg_test_acc))

    checkpoint.save(file_prefix=checkpoint_prefix)

print('Learning Finished!')
```


<br>



ì•„ë˜ì™€ ê°™ì€ functional modelë„ ê°€ëŠ¥í•˜ë‹¤. [click reference!](https://github.com/deeplearningzerotoall/TensorFlow/blob/master/lab-11-2-mnist-cnn-keras-functional-eager.ipynb)


sequentialì—ì„œì™€ ëª¨ë¸ ìƒì„± ì´ì „ì€ ë™ì¼í•˜ë‹¤


```python
def create_model():
    inputs = keras.Input(shape=(28, 28, 1))
    conv1 = keras.layers.Conv2D(filters=32, kernel_size=[3, 3], padding='SAME', activation=tf.nn.relu)(inputs)
    pool1 = keras.layers.MaxPool2D(padding='SAME')(conv1)
    conv2 = keras.layers.Conv2D(filters=64, kernel_size=[3, 3], padding='SAME', activation=tf.nn.relu)(pool1)
    pool2 = keras.layers.MaxPool2D(padding='SAME')(conv2)
    conv3 = keras.layers.Conv2D(filters=128, kernel_size=[3, 3], padding='SAME', activation=tf.nn.relu)(pool2)
    pool3 = keras.layers.MaxPool2D(padding='SAME')(conv3)
    pool3_flat = keras.layers.Flatten()(pool3)
    dense4 = keras.layers.Dense(units=256, activation=tf.nn.relu)(pool3_flat)
    drop4 = keras.layers.Dropout(rate=0.4)(dense4)
    logits = keras.layers.Dense(units=10)(drop4)
    return keras.Model(inputs=inputs, outputs=logits)

model = create_model()



```
