---
layout: post
title:  "[NLP paper] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding "
date:   2021-03-16
desc: " "
keywords: "DL, ML"
categories: [Deeplearning]
tags: [DL, ML, pytorch]
icon: icon-html
---

이번에 정리해볼 [논문](https://arxiv.org/abs/1810.04805)은,


# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

나만 빼고 다 읽어본 거 같은, 나만 빼고 다 써본 적 있는 것 같은 `BERT`,, 조금씩 읽어나가면서 수정하고 추가할 예정 !



<br>

![fig](https://miro.medium.com/max/5672/1*p4LFBwyHtCw_Qq9paDampA.png)

`BERT`는 `Bidirectional Encoder Representation from Transformer`의 약어로, 이름에서 모든 특징을 설명해주는 모델이다.

사실 `BERT`의 약어를 이루는 각각의 용어들이, 내게는 너무 모호하게 느껴지는

즉 대략적인 의미만 알지 구체적인 flow를 모르는 것들이라, 하나하나 공들여 읽어보기로 !!


<br>

아래와 같은 질문에 초점을 두고 읽었다.

```
1. transformer가 어떻게 이용되었는가? (다른 모델에서의 transformer와 다르게 사용되었는가?)
2. ELMO와의 차이는?
3. pre-training의 구체적인 flow 와 의미는?
4. 어떤 점 때문에 다양하게 활용 가능한가?
```

<br>

<br>


## Index

우선 index를 살펴보면,


<br>

```
1. Introduction
2. Related Work
 2.1 Unsupervised Feature-based Approachs
 2.2 Unsupervised Fine-tuning Approach
 2.3 Transfer Learning from supervised data
3. BERT
 3.1 Pre-traning BERT
 3.2 Fine-tuning BERT
4. Experiments
 4.1 GLUE
 4.2 SQuAD v1.1
 4.3 SQuAD v2.0
 4.4 SWAG
5. Ablation Studies
 5.1 Effect of Pre-training Tasks
 5.2 Effect of Model Size
 5.3 Feature-based Approach with BERT
6. Conclusion
# Appendix
```

3에서 보이듯, BERT의 모델은 pre-training 부분과 fine-tuning 부분으로 나뉜다.

그리고 4. Experiment에서 fine-tuning의 차이를 두고 성능을 테스트한다

5에서의 Ablation Study는 뭔지 몰라서 검색해봤는데, feature를 제거해가며 그것이 성능에 얼마나 영향을 끼치는지를 확인한다.

부록인 appendix 에서는 Additional Details for Bert, Detailed Experimental Setup, Additional Ablation Studies를 다룬다. 

<br>




<br>
