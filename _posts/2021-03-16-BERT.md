---
layout: post
title:  "[NLP paper] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding "
date:   2021-03-16
desc: " "
keywords: "DL, ML"
categories: [Deeplearning]
tags: [DL, ML, pytorch]
icon: icon-html
---

이번에 정리해볼 [논문](https://arxiv.org/abs/1810.04805)은,


# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

나만 빼고 다 읽어본 거 같은, 나만 빼고 다 써본 적 있는 것 같은 `BERT`,, 조금씩 읽어나가면서 수정하고 추가할 예정 !



<br>

![fig](https://miro.medium.com/max/5672/1*p4LFBwyHtCw_Qq9paDampA.png)

`BERT`는 `Bidirectional Encoder Representation from Transformer`의 약어로, 이름에서 모든 특징을 설명해주는 모델이다.

사실 `BERT`의 약어를 이루는 각각의 용어들이, 내게는 너무 모호하게 느껴지는

즉 대략적인 의미만 알지 구체적인 flow를 모르는 것들이라, 하나하나 공들여 읽어보기로 !!


<br>

아래와 같은 질문에 초점을 두고 읽었다.

```

1. transformer가 어떻게 이용되었는가? (다른 모델에서의 transformer와 다르게 사용되었는가?)
2. ELMO와의 차이는?
3. pre-training의 구체적인 flow 와 의미는?
4. 어떤 점 때문에 다양하게 활용 가능한가? 
```

<br>
