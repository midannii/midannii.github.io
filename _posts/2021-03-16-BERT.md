---
layout: post
title:  "[NLP paper] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding "
date:   2021-03-16
desc: " "
keywords: "DL, ML"
categories: [Deeplearning]
tags: [DL, ML, pytorch]
icon: icon-html
---

이번에 정리해볼 [논문](https://arxiv.org/abs/1810.04805)은,


# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

나만 빼고 다 읽어본 거 같은, 나만 빼고 다 써본 적 있는 것 같은 `BERT`,, 조금씩 읽어나가면서 수정하고 추가할 예정 !



<br>

![fig](https://miro.medium.com/max/5672/1*p4LFBwyHtCw_Qq9paDampA.png)

`BERT`는 `Bidirectional Encoder Representation from Transformer`의 약어로, 이름에서 모든 특징을 설명해주는 모델이다.

사실 `BERT`의 약어를 이루는 각각의 용어들이, 내게는 너무 모호하게 느껴지는

즉 대략적인 의미만 알지 구체적인 flow를 모르는 것들이라, 하나하나 공들여 읽어보기로 !!


<br>

아래와 같은 질문에 초점을 두고 읽었다.

```
1. transformer가 어떻게 이용되었는가? (다른 모델에서의 transformer와 다르게 사용되었는가?)
2. ELMO와의 차이는?
3. pre-training의 의미는?
4. 어떤 점 때문에 다양하게 활용 가능한가?
```

<br>

<br>


## Index

우선 index를 살펴보면,


<br>

```
1. Introduction
2. Related Work
 2.1 Unsupervised Feature-based Approachs
 2.2 Unsupervised Fine-tuning Approach
 2.3 Transfer Learning from supervised data
3. BERT
 3.1 Pre-traning BERT
 3.2 Fine-tuning BERT
4. Experiments
 4.1 GLUE
 4.2 SQuAD v1.1
 4.3 SQuAD v2.0
 4.4 SWAG
5. Ablation Studies
 5.1 Effect of Pre-training Tasks
 5.2 Effect of Model Size
 5.3 Feature-based Approach with BERT
6. Conclusion
# Appendix
```

`3. BERT`에서 보이듯, BERT의 모델은 `pre-training` 부분과 `fine-tuning` 부분으로 나뉜다.

그리고 `4. Experiment`에서 fine-tuning의 차이를 두고 성능을 테스트한다

`5. Ablation Study`는 뭔지 몰라서 검색해봤는데, feature를 제거해가며 그것이 성능에 얼마나 영향을 끼치는지를 확인한다.

부록인 `Appendix` 에서는 Additional Details for Bert, Detailed Experimental Setup, Additional Ablation Studies를 다룬다.

<br>






<br>


따라서 처음에 제시했던 아래 질문들에 대해서는 다음과 같이 답할 수 있겠다.




```
1. transformer가 어떻게 이용되었는가? (다른 모델에서의 transformer와 다르게 사용되었는가?)
  - <Attention is all you need> 에서의 Transformer와 동일하다.

2. ELMO와의 차이는?
  - ELMO는 left-to-right & right-to-left LSTM을 따로 학습시킨 후 concatenation하여 downstream task의 feature를 만든다.
    - 반면, BERT는 모든 Layer에 대해 left & right context를 jointly conditioned 한다.
  - ELMO는 feature-based approach 이다.
    - 반면, BERT는 fine-tuning approaches 이다.

3. pre-training의 의미는?
  - Encoder가 입력 문장들을 임베딩 하여 언어를 모델링하는 언어 모델링 구조 과정
  - pre-training을 마친 embedding 은 corpus의 의미적, 문법적 정보를 충분히 담고 있어, BERT의 등장 이후로 pre-trained model을 어떻게 사용하는지가 관건이 되었다.

4. 어떤 점 때문에 다양하게 활용 가능한가?
  - task-specific 한 inputs & output을 BERT에 끼워넣고 end-to-end로 모든 parameter를 fine-tuning함.
    - self-attention mechanism을 이용하여 concatenated text pair를 encoding 하는 것이 두 문장 사이의 bidirectional cross attention를 포함하기 때문 !
```




<br>


## 이해에 참고한 링크


- [https://velog.io/@jinml/BERT](https://velog.io/@jinml/BERT)
- [https://reniew.github.io/47/](https://reniew.github.io/47/)
- [https://towardsdatascience.com/bert-why-its-been-revolutionizing-nlp-5d1bcae76a13](https://towardsdatascience.com/bert-why-its-been-revolutionizing-nlp-5d1bcae76a13)

<br>
