---
layout: post
title:  "[ëª¨ë‘ë¥¼ ìœ„í•œ ë”¥ëŸ¬ë‹2 Pytorch] Perceptron, MLP "
date:   2021-01-16
desc: " "
keywords: "DL, ML"
categories: [Deeplearning]
tags: [DL, ML, pytorch]
icon: icon-html
---

ğŸ³ reference: <ëª¨ë‘ë¥¼ ìœ„í•œ ë”¥ëŸ¬ë‹ 2: pytorch> Lab8


<br>

# Perceptron

ì¸ê³µì‹ ê²½ë§ì˜ ëŒ€í‘œì ì¸ ì˜ˆì‹œì¸ `Perceptron `


![fig](https://www.researchgate.net/profile/Pradeep_Singh22/publication/283339701/figure/fig2/AS:421642309509122@1477538768781/Single-Layer-Perceptron-Network-Multilayer-Perceptron-Network-These-type-of-feed-forward.png)


ìœ„ ê·¸ë¦¼ì—ì„œ ì²˜ëŸ¼ ì…ë ¥ xë“¤ê³¼ ê°€ì¤‘ì¹˜ wì˜ ê³±ì˜ í•©ì— biasë¥¼ ë”í•œ í›„ activation funtionì„ ê±°ì³ ê²°ê³¼ê°€ ë‚˜ì˜¨ë‹¤.

ì´ˆì°½ê¸° perceptronì€ linear classifierì— ì“°ì˜€ë‹¤.




# XOR



ê·¸ëŸ¬ë‚˜ perceptronì˜ ê²½ìš°, `XOR` ë¬¸ì œë¥¼ í•´ê²°í•˜ì§€ ëª»í–ˆë‹¤.


![fig](https://www.researchgate.net/profile/A_Zahedi/publication/321687478/figure/fig2/AS:602499278979075@1520658430966/a-circuit-symbol-and-b-accuracy-table-of-the-XOR-logic-gate.png)


ê·¸ëŸ¬ë‚˜ XORì„ ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ êµ¬í˜„í•˜ê²Œ ë˜ë©´ì„œ ë”¥ëŸ¬ë‹ì€ ë‹¤ì‹œ bloom í–ˆë‹¤!

ì¼ë‹¨ ì½”ë“œëŠ” ì•„ë˜ì™€ ê°™ë‹¤.

```python
import torch

X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)
Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)

# nn layers
linear = torch.nn.Linear(2, 1, bias=True)
sigmoid = torch.nn.Sigmoid()
# model
model = torch.nn.Sequential(linear, sigmoid).to(device)
# define cost/loss & optimizer
criterion = torch.nn.BCELoss().to(device) # BCELoss(): binary cross entropy loss ì´ìš©
optimizer = torch.optim.SGD(model.parameters(), lr=1)

for step in range(10001):
    optimizer.zero_grad()
    hypothesis = model(X)
    # cost/loss function
    cost = criterion(hypothesis, Y)
    cost.backward()
    optimizer.step()

    if step % 100 == 0:
        print(step, cost.item())
```


<br>

# Multi Layer Perceptron (MLP)

ë‹¨ì¸µì´ ì•„ë‹ˆë¼ ì—¬ëŸ¬ ì¸µìœ¼ë¡œ ì´ë£¨ì–´ì§„ perceptronì„ ì˜ë¯¸í•œë‹¤.

![fig](https://camo.githubusercontent.com/a59e2300b0d6a24052cd6185673c59df8bae8009f365dbef31be473e209d2fb9/68747470733a2f2f7777772e7265736561726368676174652e6e65742f70726f66696c652f4d6f68616d65645f5a616872616e362f7075626c69636174696f6e2f3330333837353036352f6669677572652f666967342f41533a33373131313835303736313031323340313436353439323935353536312f412d6879706f746865746963616c2d6578616d706c652d6f662d4d756c74696c617965722d50657263657074726f6e2d4e6574776f726b2e706e67)



MLPë¥¼ ì´ìš©í•˜ì—¬ ì•„ë˜ì™€ ê°™ì´ ì„ ì„ í•˜ë‚˜ ë” ê¸‹ëŠ” ëŠë‚Œìœ¼ë¡œ XOR ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆì—ˆë‹¤.


![fig](https://i.ytimg.com/vi/kNPGXgzxoHw/maxresdefault.jpg)



ë¬¼ë¡  ì²˜ìŒì—ëŠ” MLPëŠ” êµ¬í˜„ì´ ë¶ˆê°€ëŠ¥í–ˆì§€ë§Œ, `backpropagation`ì´ ê°œë°œë˜ë©´ì„œ ê°€ëŠ¥í•´ì¡Œë‹¤.


<br>

## Backpropagation


xië“¤ë¡œ ì˜ˆì¸¡í•œ yëŠ” ì‹¤ì œ ê°’ê³¼ì˜ ì˜¤ì°¨ `loss` ë¥¼ ê°–ëŠ”ë‹¤.


![fig](https://jamesmccaffrey.files.wordpress.com/2012/11/backpropagationcalculations.jpg)

ì´ë¥¼ weightì— ëŒ€í•´ ë¯¸ë¶„í•˜ì—¬, ì´ë¥¼ ì´ìš©í•´ ë’¤ìª½ weightë¶€í„° lossë¥¼ ìµœì†Œí™”í•  ìˆ˜ ìˆë„ë¡ update í•˜ëŠ” ê²ƒì´ë‹¤.




```python
X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)
Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)

# nn layers
w1 = torch.Tensor(2,2).to(device)
b1 = torch.Tensor(2).to(device)
w2 = torch.Tensor(2,1).to(device)
b2 = torch.Tensor(1).to(device)

def sigmoid(x):
  return 1.0/(1.0+torch.exp(-x))

def sigmoid_prime(x):
  return sigmoid(x)*(1-sigmoid(x))

for step in range(10001):
  # forward
  l1 = torch.add(torch.matmul(X, w1), b1)
  a1 = sigmoid(l1)
  l2 = torch.add(torch.matmul(a1, w2), b2)
  Y_pred = sigmoid(l2)

  cost = -torch.mean(Y*torch.log(Y_pred) + (1-Y)*torch.log(1-Y_pred))

  # Backpropagation (chain rule)
  ## Loss derivative
  d_Y_pred = (Y-Y_pred) / (Y_pred * (1.0 - Y_pred) + 1e-7)

  # Layer 2
  d_l2 = d_Y_pred * sigmoid_prime(l2)
  d_b2 = d_l2
  d_w2 = torch.matmul(torch.transpose(a1, 0, 1), d_b2)

  # Layer 1
  d_a1 = torch.matmul(d_B2, torch.transpose(w2, 0, 1))
  d_l1 = d_a1 * sigmoid_prime(l1)
  d_b1 = d_l1
  d_w1 = torch.matmul(torch.transpose(X, 0, 1), d_b1)

  # weight update
  w1 = w1-learning_rate * d_w1
  b1 = b1 - learning_rate * torch.mean(d_b1, 0)
  w2 = w2-learning_rate * d_w2
  b2 = b2 - learning_rate * torch.mean(d_b2, 0)

  if step%100 == 0: print(step, cost.item())
```


<br>

ì´ëŸ¬í•œ backpropagationì„ ì´ìš©í•œ XOR ì½”ë“œëŠ” ì•„ë˜ì™€ ê°™ë‹¤.

```python
X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)
Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)

# nn layers
linear1 = torch.nn.Linear(2, 2, bias=True)
linear2 = torch.nn.Linear(2, 1, bias=True)
sigmoid = torch.nn.Sigmoid()

# model
model = torch.nn.Sequential(linear1, sigmoid, linear2, sigmoid).to(device)

# define cost/loss & optimizer
criterion = torch.nn.BCELoss().to(device)
optimizer = torch.optim.SGD(model.parameters(), lr=1)  # modified learning rate from 0.1 to 1

for step in range(10001):
    optimizer.zero_grad()
    hypothesis = model(X)

    # cost/loss function
    cost = criterion(hypothesis, Y)
    cost.backward()
    optimizer.step()

    if step % 100 == 0:
        print(step, cost.item())
```



ë˜ëŠ” ë” ê¹Šì€ layerë¥¼ ì•„ë˜ì™€ ê°™ì´ ì„¤ì •í•  ìˆ˜ ìˆë‹¤. ì´ ê²½ìš° ì„±ëŠ¥ì´ ë” í–¥ìƒëœë‹¤.

```python
# nn layers
linear1 = torch.nn.Linear(2, 10, bias=True)
linear2 = torch.nn.Linear(10, 10, bias=True)
linear3 = torch.nn.Linear(10, 10, bias=True)
linear4 = torch.nn.Linear(10, 1, bias=True)
sigmoid = torch.nn.Sigmoid()

# model
model = torch.nn.Sequential(linear1, sigmoid, linear2, sigmoid, linear3, sigmoid, linear4, sigmoid).to(device)
```

<br>
