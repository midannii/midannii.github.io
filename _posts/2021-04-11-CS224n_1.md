---
layout: post
title:  "[Stanford/CS224n] [1] Word Vectors"
date:   2021-04-11
categories: CS224n 
---

<br>

드디어 1강, *Word Vectors* !!

단어를 vector로 나타내는 두가지 방법, count-based model과 neural network model을 소개한다.



<br>

사람의 언어는 `signifier(symbol)`에 mapping된 `signified(idea, thing, ..)`로,

meaning을 전달하기 위해 word, phrase, 등을 사용한다.


NLP란, 이러한 사람의 언어를 컴퓨터로 하여금 이해할 수 있게 알고리즘을 설계하는 것이다.

컴퓨터에서 단어를 표현하기 위해서 `wordnet`과 같이 유의어와 상의어 시소러스(사전)를 활용하기도 한다.

<br>

그런데 이 과정에서 뉘앙스, 새로운 의미, 신조어 등등

놓치는 부분이 당연히 생길 수 밖에 없고, 단어의 유사도도 알아내기 힘들다.


<br>

음 더 자세하게 단어를 컴퓨터가 이해하기 쉽게(=0과 1로) 표현하는 방법을 살펴보자.


<br>

## One-hot vector

단어를 vector로 표현할 수 있는 가장 간단한 방법으로,

각 단어들을 해당 단어의 index에만 1, 나머지는 0인 vector로 나타낸다.

이때 단어의 유사도는 알 수 없으며, 단어의 개수가 많을수록 차원의 수가 커진다.


![fig](https://i.imgur.com/mtimFxh.png)



<br>


## Distributional Hypothesis

동일한 context 내에서 사용되는 단어들은 비슷한 의미를 갖는다는 가설로,

단어 w가 출연할 떄, window size 내 단어들의 집합인 w의 context는 주위에 나타나며,

w의 많은 context를 통해 w의 representation을 생성한다.



<br>

word representation은 아래와 같이 분류된다.

![fig](https://blog.kakaocdn.net/dn/cXS4ru/btqKwrAqtZe/4p5DE6jdPdX75S5nQ25vO0/img.jpg)


이 중에서도, count-based model인 `SVD`, neural network model에서는 `word2vec`이 유명하다.


<br>

# SVD: singular value decomposition


SVD가 nlp에도 쓰이는구나,, 싶었다!

![fig](https://lena-voita.github.io/resources/lectures/word_emb/preneural/idea-min.png)


각 문장에 대해 window size = 1인 co-occurence matrix를 생성하고,

SVD으로 차원축소를 진행한 후, M = U * sigma * V에서의 U가 각 word vector가 된다.


이 방법은 새로운 단어가 출현할때마다 matrix의 차원이 바뀌므로 매번 다시 계산해주어야 하고,

대부분 단어가 co-occur하지 않아 matrix가 sparse하며 높은 차원을 가지므로 cost가 높다.


그래서 이를 보완하기 위해 stop words를 제거하고, ramp window(단어의 거리가 떨어짐에 따라 다른 weight를 줌)를 적용하거나,

co-occurence matrix를 만들떄 단순 count말고 pearson correlation이나 negative count를 이용함으로서 이를 어느정도 보완한다.


<br>

# word2vec

![fig](https://devopedia.org/images/article/221/4080.1570464995.png)

word2vec에는 `CBow`, `skip-gram` 의 두가지 알고리즘이 있다.



<br>

## CBow

주변 context 단어들로부터 하나의 target 단어를 예측하는 모델로, 단어의 순서를 고려하지 않는다.

![fig](https://www.researchgate.net/profile/Daniel-Braun-6/publication/326588219/figure/fig1/AS:652185784295425@1532504616288/Continuous-Bag-of-words-CBOW-CB-and-Skip-gram-SG-training-model-illustrations.png)


## Skip-gram

하나의 target으로부터 주위 context를 예측하는 모델이다.


![fig](https://miro.medium.com/max/2231/0*yxs3JKs5bKc4c_i8.png)

<br>

추가적으로, computational efficiency를 향상시키기 위해서는

1. Subsampling of Frequent words

- 자주 등장하는 단어는 학습을 덜 시키기 (the, a, in 등등)

    - 단어들의 출현 빈도에 대해 아래와 같은 subsampling 적용

        - 해당 단어가 subsampling될 확률 `P(wi)`

        - 단어의 frequency `wi`, threshold  `t` = 10^-5

![fig](https://i.stack.imgur.com/Eq2u8.png)


2. Hierarchical Softmax

- 마지막 softmax 층에서, 모든 단어 |V|를 binary tree로 나타낸 Hierarchical Softmax를 사용하는 것

- 계산양이 log|V|로 감소함 !!

- [여기](https://seunghan96.github.io/ne/03.Hierarchical_Softmax/)에 설명이 잘 되어있당 !!

    - 각 node, child에 대해, 왼쪽 child node 에게는 1, 오른쪽 child node에게는 -1을 주는 방식

    - 이때 각각 좌우 child node가 될 확률

![fig](https://webcms3.cse.unsw.edu.au/static/uploads/pic/z5125251/7462c15cf5c288fd116650be2c2a93796daf10c59eeb6d7570b1d12d3cb6c2e0/2019-11-223.56.07.png)

![fig](https://paperswithcode.com/media/methods/main-qimg-4ca2bfbef8d1a278c65efc1be5a3965d_mCzj4rb.png)

3. Negative Sampling

- '좋은 모델은 `logistic regression`을 통해 noise를 구분할 수 있어야 한다'를 가정

    - `Noise Contrastive Estimation`의 변형

![fig](https://image.slidesharecdn.com/wordrepresentationsinvectorspace-150420222859-conversion-gate02/95/word-representations-in-vector-space-18-638.jpg?cb=1429586991)

- 흔한 단어일수록 현재 target 단어와 더 연관성이 적을 것이라고 보고, 자주 등장하는 단어일수록 연관성이 낮을 것이라고 본 후 target 단어와 연관성이 없을 것이라고 추정되는 단어를 뽑는 것

- 현재 문장에 없는 단어를 전체 데이터 셋에서 추출해내는 것.


<br>

# Optimization

cost function을 optimization하기 위해 `gradient descent` 방법을 사용한다.

<br>

# Reference

- [강의](https://www.youtube.com/watch?v=8rXD5-xhemo&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z)

- [한글 강의](https://www.youtube.com/watch?v=9woiID8QzbE&list=PLetSlH8YjIfVdobI2IkAQnNTb1Bt5Ji9U)

- [자료 ](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/)


- [참고](http://solarisailab.com/archives/959)
<br>
