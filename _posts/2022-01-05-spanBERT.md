---
layout: post
title:  [NLP] SpanBERT: Improving Pre-training by Representing and Predicting Spans
date:   2022-01-05
desc: " "
keywords: "DL, ML"
categories: [Deeplearning]
tags: [DL, ML]
icon: icon-html
---

## * 요약
>
- spanBERT == text의 span을 더 잘 representation 및 predict할 수 있는 PLM model
- 기존 BERT의 연장선

    (1) 기존 BERT는 token 단위 masking을 했는데, SpanBERT에서는 여러 단어에 걸쳐 있는 token들을 같이 masking (continuous token masking)

    (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it.

- spanBERT는 기존 BERT보다 span selection task(e.g. QA, conference resolution)에서 뛰어난 성능을 보임

## 1.  Introduction

- NLP task에서 multiple span selection하는 경우가 많음
- multiple span selection task에서의 성능향상을 위해, 기존 BERT에서 두가지를 변경한 spanBERT제안함

    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/47717a72-659f-480a-a546-544bcc0fe446/Untitled.png)

    - BERT와 다른 masking scheme

        즉, random individual token을 masking 하는게 아니라 random contiguous spans을 masking함

    - BERT와 다른 training objective

        span-boundary objective (SBO)

        - 모델이 token의 Span의 boundary 에 있는 토큰들로 span 안의 토큰들을 예측하도록 학습됨
        - 모델이 boundary token에서의  span-level information를 저장함으로서, fine-tuning단계에서 이를 쉽게 access할 수 있게 함
- baseline에서 `single-sequence BERT` 추가했음
    - Next Sentence Prediction에서 two half-length segments보다 single segment에서 성능이 좋았기 때문
- 다양한 QA나 coference resolution에서 성능 향상하였음
- 기존 연구가 데이터 양이나 모델 크기를 증가하면서 성능 향상을 꾀했다면, 본 논문에서는 좋은 pre-training tasks & objectives를 통해 성능을 향상함

## 2. Background: BERT

- transformer encoder 구조로 pre-training 하는 self-supervised approach임
- pre training task로 MLM, NSP사용
- BERT optimizes the MLM and the NSP objectives by masking word pieces uniformly at random in data generated by the bi-sequence sampling procedure. In the next section, we will present our modifications to the data pipeline, masking, and pre-training objectives.
