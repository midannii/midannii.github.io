---
layout: post
title:  "[NLP] NumNet: Machine Reading Comprehension with Numerical Reasoning  "
date:   2021-11-11
categories: PaperReview
---

## * 간단하게 요약

기존 MRC에서 잘 다뤄지지 않았던 Numerical reasoning (+, -, 정렬, counting) 을 해결하기 위한 NumNet model 제안

NumNet은?

- Graph Neural Network(GNN)을 통해 comparing information를 고려하고 질문과 구절의 숫자에 대해 numerical reasoning을 수행함
    - reasoning module 사용함
    - NumGNN을 사용하여, 숫자를 node로, 비교관계를 edge의 화살표로 나타냄

        → reasoning module에서의 one-step reasoning를 사용하여 node representation을 update하며 reasoning

- DROP dataset에서 EM-score 64.56% 달성함 (SOTA)
- 아래와 같은 약점 존재
    - 여러개의 gold answer을 가지며, 그 answer끼리 인접하지 않은 경우
    - document/question에서 문자 그대로 발생하지 않지만 counting또는 arithmetic operation에서 파생되어야 하는 중간 숫자와의 비교

---

## 1. Introduction

- NAQANet은 3가지 종류의 numerical MRC을 가능하게 했음

    (1) extracting spans; (2) counting; (3) addition or subtraction

    - 그러나, numerical reasoning을 명확하게 고려하지 않았음

        → 기존 MRC 모델에 수치 추론을 통합하는 NumNet을 제안

- numerical reasoning이 필요한 질문에 답하기 위해서 numerical comparison을 수행해야 함

    (1) 숫자를 비교하는 numerical comparison, (2) 글을 해석해서 부분(e.g. %)을 파악하는 numerical condition

    → NumNet은 숫자 간의 숫자 비교 정보를 고려함

- multi-layer NumGNN(numerically-aware graph neural network)을 통해 sorting이 필요한 질문에 효과적으로 답할 수 있음
    - encoding후 question representation과 passage representation을 NumGNN에 넣음

        → 이를 통해 숫자 간의 비교 정보를 representation에 추가로 통합


## 2. Related work

- Machine Reading Comprehension
    - numerical MRC를 위한 NAQANet
        - QANet의 output layer를 변경한 것임
        - encoding, inference 동안은 Numerical reasoning 수행하지 않음
- Arithmetic Word Problem Solving
    - 기존의 AWP model은 small benchmark dataset에서만 학습 & valid됨

## 3. Methodology

![Untitled](https://d3i71xaburhd42.cloudfront.net/20632d08b69591c3c4bd6661193a226de35fcc3a/3-Figure1-1.png)

### 3.1. Framework

- NAQANet을 base model로 사용함
- encoding module과 prediction module 사이에 reasoning module을 사용
    - reasoning module에서는, NumGNN을 활용하여 수치 비교 정보를 명시적으로 고려하고 수치 추론을 수행함

[encoding module]

- QANet, NAQANet에서의 encoding module 사용
- convolution layer, self-attention layer, feed-forward layer와 passage-question Attention layer로 구성된 인코딩 모듈을 사용하여 question, paragraph를 각각 모두 encoding함
    - 먼저 각 Q,P의 stacked embedding encoder layer을 거치고,

        P와 Q, Q와 P의 context-query attention layer을 거침


[reasoning module]

- graph neural network를 기반으로 reasoning
    - heterogeneous directed graph G = (V ; E) 를 먼저 정의함
        - node (V)는 question, passage에 있는 number랑 상응
            - 각각 Vq, Vp 이며, V는 이 둘의 합집합
        - edge (E)는 number들의 numerical relationship을 encoding하는 데에 사용
    - 앞서 계산한 P-, Q-와 weight matrix인 W로 QANet의 encoding을 거침

        → MQ, MP, 그리고 number에 상응하는 representation인 U를 도출함

- NumNet은 NAQANet과 달리 reasoning module이 있어서 numerical comparison이 가능함

    → numerical reasoning을 더 잘 수행할 수 있음


[prediction module]

- prediction 자체는 NAQANet과 거의 동일함
- NAQANet에 따라, 답변을 4가지 유형으로 나누고 고유한 output layer를 사용하여 각 유형에 대한 조건부 답변 확률 Pr(answer|type)을 계산
    - passage span; 답은 Passage 의 Span 이며, 정답 확률은 start position와 end position의 확률을 곱한 값으로 정의됨
    - question span; 답은 quenstion의 span이며, 정답 확률은 start position와 end position의 확률을 곱한 값으로 정의됨
    - count; counting을 통해 답을 얻음. 10개의 숫자(0-9)에 대한 다중 클래스 분류 문제
    - Arithmetic expression; arithmetic expression(산술식)을 통해 답을 얻음. 아래와 같은 3가지의 산술식 존재

        (1) extract all numbers from the passage

        (2) assign a sign (plus, minus or zero) for each number

        (3) sum the signed numbers.


### 3.2. Numerically-aware Graph Construction

- G = (V, E)
    - edge의 방향으로 대소관계 표현함
        - n(a) > n(b)이면 a → b, 그 반대면 a ← b

### 3.3. Numerical Reasoning

NumNet이 reasoning 잘 수행하기 위해 graph 구조 사용

- initialization ; Mp, viP, Mq, viq
- one-step reasoning

     아래와 같은 3단계의 reasoning을 위해 GNN 사용함

    (1) Node Relatedness Measure; reasoning에 무관한 애들 지움

    → node vi의 weight인 alpha i를 도출함 (sigmoid 사용)

    (2) Message Propagation

    - reasoning에서 number 뿐만 아니라 context도 같이 고려하기 위해,  각 node에서 이웃으로 message를 propagation함

        → 이를 위해, relation-specific transform matrices 사용

        → node vi의 message representation인 v~'i를 도출함

    - 이때 edge eji의 relation인 rij는 4가지 종류를 가짐

        (1) both from the question (q-q)

        (2) both from the passage (p-p)(

        3) from the question and the passage respectively (q-p)

        (4) from the passage and the question respectively (p-q).


    (3) Node Representation Update

    이전 단계에서 얻은 메시지 표현은 이웃의 정보만 포함하므로 노드 표현과 융합하여 노드 자체가 전달하는 정보와 결합해야 함

    →RELU 사용


## 4. Experiments

### 4.1. Dataset and Evaluation Metrics

- DROP dataset 사용
- 2가지 evaluation metrics 사용
    - Exact Match (EM)
    - numerically-focused F1 scores
        - 0 when predicted answer is mismatched for those questions with the numeric golden answer.

### 4.2. Baselines

- semantic parsing models ; KDG, OpenIE, SRL
- traditional MRC models; BiDAF, QANet
- numerical MRC models; NAQANet, NAQANet+

### 4.3. Experimental Settings

### 4.4. Overall Results

DROP에서 우리꺼 성능 좋다 ~ NAQANet보다 성능 좋다 ~

- DROP의 comparing dataset이 bias되어있어, dev set의 comparing questions을 crowdsourcing을 통해 재구축함

### 4.5. Effect of GNN structure

- NumGNN은 GNN보다 유의미한 성능 향상을 유발

    → 죽, 숫자보다 비교 정보를 고려하는 것이 비교 질문에 대한 numerical reasoning에 효과적으로 도움이 될 수 있음

- DROP dataset에서 comparisoning quesiton이 bias 되었음

    → 크라우드 소싱으로, dev set의 해당 부분 재건축함


### 4.6. Effect of GNN Layer Number

NumGNN 레이어의 수는 우리 모델의 수치적 추론 능력을 나타냄 (K-layer 버전에는 K-step numerical inference이 있음)

(1) NumNet의 2-layer 버전은 비교 질문에 대해 최고의 성능을 달성

(2) 전체 dev set에서 NumNet 모델의 성능은 GNN layer number가 증가함에 따라 일관되게 향상됨

→ 그러나, layer number에 따라 성능이 크게 차이나지 않았듬 ..

### 4.7. Case study

NumNet model could give the correct answer through the numeric reasoning, which indicates the effectiveness of our NumNet model.

### 4.8. Error Analysis

(1) 우리의 NumNet 모델은 sorting/comparison question의 약 76%에 정확하게 답할 수 있음.

→ NumNet 모델이 어느 정도 확장된 수치 추론 능력을 달성했음을 나타냅니다.

(2) 틀린 sorting/comparison question 중 가장 많은 질문(26%)은 golden answer이 여러 개의 인접하지 않은 범위인 질문이고,

두 번째로 많은 오답(19%)은 document/question에서 문자 그대로 발생하지 않지만 counting또는 arithmetic operation에서 파생되어야 하는 중간 숫자와의 비교를 포함하는 질문임

### 4.9. Discussion

- NumNet에서 사용하는 numerically-aware graph는 숫자를 노드로 인코딩하고 노드 간의 관계를 가장자리로 인코딩하여 비교 연산 수행
- numerically-aware graph가 미리 정의되어 있기 때문에 우리 모델의 주요 한계인 추론 과정에서 중간 숫자를 도출해야 하는 경우(예: 산술 연산)에는 NumNet을 적용할 수 없음
