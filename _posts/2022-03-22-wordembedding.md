---
layout: post
title:  "[NLP/ê°•ì˜ë‚´ìš©ì •ë¦¬] Word Embedding "
date:   2022-03-22
desc: " "
keywords: "DL, ML"
categories: [Deeplearning]
tags: [DL, ML, NLP]
icon: icon-html
---
# 1. word embedding

### one-hot encoding

![https://miro.medium.com/max/1400/1*ggtP4a5YaRx6l09KQaYOnw.png](https://miro.medium.com/max/1400/1*ggtP4a5YaRx6l09KQaYOnw.png)

- vocab size |V|ì— ëŒ€í•´, one-hot vector vëŠ”  $ğ‘£ âˆˆ 0,1^V$

    â†’ ì´ì— ë”°ë¼, ì°¨ì›ì˜ ìˆ˜ê°€ ë§¤ìš° í¼ [curse of dimensionality]

    â†’ Information quality ë‚®ì•„ì§ , Memory useâ†‘, Computation time â†‘

- curse of dimensionality
    - ëŒ€ë¶€ë¶„  MLì—ì„œ dataì˜ ì°¨ì›ìˆ˜ê°€ ë†’ì„ë•Œ ë¬¸ì œ ë°œìƒí•¨
        - ì°¨ì›ì´ ì¦ê°€í• ìˆ˜ë¡, dataê°€ ê¸‰ê²©í•˜ê²Œ sparseí•´ì§
        - subspace ë¼ë¦¬ì˜ combinationì¡°í•©ë„ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¦ê°€í•¨

    â†’ ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ì°¨ì›ì¶•ì†Œ: PCA, LDA

    - data distributionì„ ê°€ì¥ ì˜ ì„¤ëª…í•´ì£¼ëŠ” 2ê°œì˜ axisë¡œ ë°ì´í„°ë¥¼ ë‚˜íƒ€ë‚´ëŠ” PCA
- manifold hypothesis
    - ê³ ì°¨ì›ì˜ natural dataë¡œë¶€í„°, ì €ì°¨ì›ì˜ manifoldë¥¼ ì°¾ì !
    - ìˆ˜í•™ì ìœ¼ë¡œ ì¦ëª…ë˜ì§€ ì•ŠëŠ” assumptionì—ì„œ ë°œê²¬ë¨
    - ì‹¤ì œ dataëŠ” uniformí•œ ë¶„í¬ê°€ ì•„ë‹ˆê³ , nonlinear patternì„
        - ì´ manifoldëŠ” original spaceì—ì„œ ì°¾ê¸° í˜ë“¦ : nonlinearí•´ì„œ, nonlinear activation ì‚¬ìš©í•´ì•¼í•¨

            â†’ ë³µì¡í•œ nonlinearì‚¬ì´ì— linearê°€ í¬í•¨ë˜ì–´ ìˆê¸° ë•Œë¬¸ì—, NNì—ì„œ linearì™€ nonlinear functionì„ ë²ˆê°ˆì•„ê°€ë©´ì„œ ì‚¬ìš©í•¨

    - how to capture manifold?

        â†’ autoencoder ì‚¬ìš©

        ![https://media.vlpt.us/images/jaehyeong/post/36b7d4e6-1193-4ab1-8f21-d085c162db24/denoising-autoencoder-architecture.png](https://media.vlpt.us/images/jaehyeong/post/36b7d4e6-1193-4ab1-8f21-d085c162db24/denoising-autoencoder-architecture.png)

        - goal: input dataì˜ low dimensional representation ì°¾ê¸°
            - bottleneckì€, featureê°€ í•©ì³ì§„ í˜•íƒœì„
            - bottleneck vectorë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ ? : ë‹¨ìˆœíˆ data ë¶„ì„ì„ í•˜ê¸° ìœ„í•´ì„œ or ëª¨ë“  networkë¥¼ í•™ìŠµí•˜ë ¤ë©´ dataê°€ ì—„ì²­ ë§ì´ í•„ìš”í•œë°, íš¨ê³¼ì ì¸ í•™ìŠµì„ ìœ„í•´
            - challengeë¥¼ ì£¼ê¸° ìœ„í•œ bottleneck ì¦‰ ë„ˆë¬´ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ì•ˆë¨
        - encoder:  ê³ ì°¨ì›ì˜ inputì„ ì €ì°¨ì›ìœ¼ë¡œ translation
        - decoder: dataë¥¼ recoverí•˜ì—¬ input dataì™€ ë™ì¼í•œ í¬ê¸°ë¡œ ë§Œë“¦
        - loss function: MSE ì‚¬ìš© (reconstruction error) : ì¦‰ original dataì™€ manifold planeì—ì„œì˜ ì¢Œí‘œ(ì¦‰ encoded input)ì‚¬ì´ì˜ ìœ í´ë¦¬ë””ì–¸ ê±°ë¦¬

â†’ ë”°ë¼ì„œ, one-hot embeddingì— autoencoderë¥¼ ì ìš©í•˜ë©´??

â†’ ë” denseí•´ì§€ê³ , informativeí•´ì§

## Word Embedding: Word2Vec

- Distributional Hypothesis : Words that occur in similar contexts tend to be similar
- unsupervised methodì„
- CBOW, Skipgram ë“±ë“±
- ì „ì²´ normalization factorë¥¼ ë‹¤ ê³„ì‚°í•˜ë©´ ë„ˆë¬´ expensiveí•¨

    â†’ ì´ë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•œ ë‹¤ì–‘í•œ techniques

    - hierarchical softmax : word ë¥¼ treeë¡œ êµ¬ì„±í•´ë³´ë©´, ì ì ˆí•œ ë‹¨ì–´ë¥¼ ì°¾ëŠ” ê³¼ì •ì´ binary searchì„ â†’ log scaleë¡œ computational expense ê°ì†Œ
    - negative sampling : randomí•˜ê²Œ samplingí•´ë„ í™•ë¥  ë¶„í¬ ìœ ì¶” ê°€ëŠ¥

### Word2Vec:CBOW

- Continuous Bag of Words (CBOW)

    ![https://www.frontiersin.org/files/Articles/458535/frobt-06-00153-HTML/image_m/frobt-06-00153-g001.jpg](https://www.frontiersin.org/files/Articles/458535/frobt-06-00153-HTML/image_m/frobt-06-00153-g001.jpg)

    - goal: ì£¼ì–´ì§„ neighboring words (contexts)ë¡œë¶€í„°, context ì •í™©ìƒ ì ì ˆí•˜ê²Œ ë“¤ì–´ê°ˆ ê²ƒ ê°™ì€ single wordsë¥¼ predict
    - Input: a set of one-hot vectors of given context words
        - ìœ„ ê·¸ë¦¼ì—ì„œ, n x kì˜ W1ëŠ” learnable parameterì„
            - k : the number of words in vocabulary (hyperparameter)
            - n : the size of hidden representation (hyperparameter)
    - ê° W1ì€ k x 1ì°¨ì›ì˜ vectorì™€ ê³±í•´ì ¸, hidden layerì˜ representationì´ ë¨

        â†’ one-hot vectorëŠ” ê±°ì˜ 0ì´ë¼ì„œ, 1ì´ë‘ ê³±í•´ì§€ëŠ” ë¶€ë¶„ë§Œ ì‚´ì•„ë‚¨ìŒ (ì¦‰ í•´ë‹¹ë˜ëŠ” indexì˜ ê°’ë§Œ ë½‘ì•„ì˜¤ëŠ” ê²ƒê³¼ ê°™ìŒ)

    - encoderë¥¼ ì‚¬ìš©í•˜ì—¬ inputì„ ì••ì¶•í•œ í›„ ë‹¤ì‹œ decoderë¡œ
    - cross-entropy loss function
        - outputì€, ê°€ì¥ ì í•©í•œ wordë¥¼ ì°¾ëŠ” classificationëŠë‚Œì„

            â†’ ì´ë ‡ê²Œ word vocabìœ¼ë¡œ classificationí•˜ëŠ”ê±¸ language modelì´ë¼ê³  í•¨ !!


### Word2Vec:Skip-gram

- example sentence: the quick brown fox funs away

    ![https://miro.medium.com/max/560/1*g1yE9fEP6pB2Yn5vUUxOTw.png](https://miro.medium.com/max/560/1*g1yE9fEP6pB2Yn5vUUxOTw.png)

- CBOWë‘ ê±°ì˜ ë˜‘ê°™ìŒ

    ![https://www.researchgate.net/profile/Wang-Ling-16/publication/281812760/figure/fig1/AS:613966665486361@1523392468791/Illustration-of-the-Skip-gram-and-Continuous-Bag-of-Word-CBOW-models.png](https://www.researchgate.net/profile/Wang-Ling-16/publication/281812760/figure/fig1/AS:613966665486361@1523392468791/Illustration-of-the-Skip-gram-and-Continuous-Bag-of-Word-CBOW-models.png)


### CBOW vs Skip-gram

- ë‘˜ì¤‘ì— skip-gramë¥¼ ë” ë§ì´ ì‚¬ìš©í•¨
- skip gram: í•™ìŠµì— ë§ì€ ì‹œê°„ì´ í•„ìš”í•¨ & ì ì€ ë°ì´í„°ë¡œë„ ì„±ëŠ¥ ì¢‹ìŒ & í¬ê·€í•œ word,phaseë„ ì˜ ì˜ˆì¸¡í•¨
- CBOW: ìì£¼ ì‚¬ìš©í•˜ëŠ” ë‹¨ì–´ë“¤ì— ëŒ€í•´ ì •í™•ë„ê°€ ë” ë†’ìŒ

### ì´ì „ì— ì‚¬ìš©í–ˆë˜ ë°©ë²•ë“¤

- edit distance
- WordNet

---

### Must-read papers

- <Efficient Estimation of Word Representations in Vector Space>, 2013, archieved
    - similar wordë¥¼ vector ê³µê°„ì—ì„œ ì •ì˜í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ syntax, semantic ê´€ì ì—ì„œ ì •ì˜ ë° ì„¤ëª…
    - NNLM(Feedforward neural net language model) ëª¨ë¸ ì œì•ˆ
- <Distributed Representations of Words and Phrases and their Compositionality>, 2013, neurips
    - skip-gramì˜ vector representation qualityë¥¼ í–¥ìƒí•˜ê¸° ìœ„í•œ 3ê°€ì§€ ë°©ë²• ì œì•ˆ
- <GloVe: Global Vectors for Word Representation>, 2014, EMNLP
    - global word-word-co-occurrence count ë¥¼ ì´ìš© ì¦‰ ì „ì²´ corpusì— ëŒ€í•œ í†µê³„ì ì¸ ì •ë³´ë¥¼ ëª¨ë‘ ë‹´ê³  ìˆëŠ” ëª¨ë¸ì¸ GloVEì œì•ˆ

---

### Supplementary readings

- [https://wikidocs.net/22660](https://wikidocs.net/22660)
- [https://simonezz.tistory.com/35](https://simonezz.tistory.com/35)

---

## Reference

- ì—°ì„¸ëŒ€í•™êµ ì—¬ì§„ì˜ êµìˆ˜ë‹˜ì˜ â€˜AAI5003: ìì—°ì–´ì²˜ë¦¬ë¥¼ ìœ„í•œ ë”¥ëŸ¬ë‹' , 2022ë…„ 1í•™ê¸°
- figures
    - [https://miro.medium.com/max/560/1*g1yE9fEP6pB2Yn5vUUxOTw.png](https://miro.medium.com/max/560/1*g1yE9fEP6pB2Yn5vUUxOTw.png)
    - [https://miro.medium.com/max/1400/1*ggtP4a5YaRx6l09KQaYOnw.png](https://miro.medium.com/max/1400/1*ggtP4a5YaRx6l09KQaYOnw.png)
    - [https://www.researchgate.net/profile/Wang-Ling-16/publication/281812760/figure/fig1/AS:613966665486361@1523392468791/Illustration-of-the-Skip-gram-and-Continuous-Bag-of-Word-CBOW-models.png](https://www.researchgate.net/profile/Wang-Ling-16/publication/281812760/figure/fig1/AS:613966665486361@1523392468791/Illustration-of-the-Skip-gram-and-Continuous-Bag-of-Word-CBOW-models.png)
    - [https://www.frontiersin.org/files/Articles/458535/frobt-06-00153-HTML/image_m/frobt-06-00153-g001.jpg](https://www.frontiersin.org/files/Articles/458535/frobt-06-00153-HTML/image_m/frobt-06-00153-g001.jpg)
