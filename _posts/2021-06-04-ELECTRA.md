---
layout: post
title:  "[NLP] ELECTRA model "
date:   2021-06-04
desc: " "
keywords: "DL, ML"
categories: [Deeplearning]
tags: [DL, ML, pytorch]
icon: icon-html
---

이번에 소개할 model은 2020년 google이 발표한 [ELECTRA](https://arxiv.org/abs/2003.10555)이다.

ELECTRA는 Baseline인 BERT를 어떻게 하면 '가볍게, 그러나 성능은 그대로' 할 수 있을지에 초점을 두었다.


<br>

# 1. Introduction


<br>

# 2. Method

<br>

# 3. Experiments

## 3.1 Experimental setup

## 3.2 model extensions

## 3.3 small models

## 3.4. large models

## 3.5 Efficiency Analysis



<br>

# 4. Related Work


## Self-supervised Pre-training for NLP

- BERT, XLNET, tinyBERT, ...

## Generative Adversarial Networks

- GAN, MaskGAN, ...

## Contrastive Learning

- NCE(Noise-Contrastive Estimation), Word2Vec, CBOW, ...

<br>

# 5. Conclusion

- 본 논문은 language representation learning에 새로운 self-supervised task 를 제시했음

- key idea는, generator network로부터 생성되는 token이 negative sample인지 input token인지를 학습시키는 것이다.

- more compute-efficient, better performance

<br>

<br>
