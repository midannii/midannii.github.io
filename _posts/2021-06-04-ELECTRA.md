---
layout: post
title:  "[NLP] ELECTRA model "
date:   2021-06-04
desc: " "
keywords: "DL, ML"
categories: [Deeplearning]
tags: [DL, ML, pytorch]
icon: icon-html
---

이번에 소개할 model은 2020년 google이 발표한 [ELECTRA](https://arxiv.org/abs/2003.10555)이다.

ELECTRA는 Baseline인 BERT를 어떻게 하면 '가볍게, 그러나 성능은 그대로' 할 수 있을지에 초점을 두었다.


<br>

# 1. Introduction

- BERT와 같은 Masked Language Model (MLM) pre-training method는 성능은 좋지만 compute가 많이 필요하다.

- 본 논문에서는 보다 sample-efficient한 pre-training task인 `replaced token detection`을 제시한다.

  - small generator network로부터 나오는 alternative sample들로 몇가지 token을 replace한 후(`Generator`), 각 token이 replaced token인지 여부를 predict(`Discriminator`) 하도록 한다.

![fig](https://blog.kakaocdn.net/dn/bgboen/btqEmwAnKsz/grrzxki3KWKPPvV14093yk/img.png)

- 확실히 ELECTRA는 적은 FLOPs(Floating Point Operations)로도 성능이 좋다

    - 기존 bert가 15%만 masking 한것에 비해, ELECTRA는 all input token으로부터 학습을 하기 때문에 *much faster & more efficient*



<br>

# 2. Method

![fig](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQtsMvMeUtcCKJ4e7ThxlXGgaf-niXphGQ3RuwBfsouhnxb46CLudEdxhSR1Lv0DjN7J2c&usqp=CAU)

- generator와 discriminator는 Transformer network등등의 encoder로 이루어져 있다.

   - encoder는 input tokens x = [x1, .. , xn]에 대해, contextualized vector representation인 h(x) = [h1, ... , hn]으로 mapping한다.


![fig](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FzMxik%2FbtqEnhbMFcH%2FAlPiEIq4Bv9tNcSei5znvk%2Fimg.png)


- *Generator* ; MLM을 수행하도록 training 된다



  - `PG(xt|x)`는 position t에서, particular token xt을 만들어낼 확률 (xt= [MASK])

  - 먼저 주어진 x = [x1, ... , xn]에 대해 m = [m1, ... mk]로 mask out하는 position의 random set을 select한다

    - select된 position들은 [MASK] token으로 대체됨 (일반적으로 15%) (`x_masked`)

  - 이후 generator가 masked-out token의 original identity를 predict하도록 학습한다.


- *Discriminator* ; token xt가 진짜인지를 predict한다.

![fig](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fdpncrr%2FbtqElWfhpID%2FqfiQCHQNSZ9RCI6KLbwj81%2Fimg.png)
  - masked-out token을 generator sample로 replace하여 corrupted example을 형성한다 (`x_corrupt`)

  - 이후 discriminator가 `x_corrupt`의 어떤 token이 original input x와 match되는지 predict하도록 학습한다

<br>

- ELECTRA는 `Generator`&`Discriminator`로 이루어져 있지만 `GAN`과는 매우 다르다.

  - text를 `GAN`에 적용하기 힘드므로, maximum likelihood로 generator를 학습시킴 (즉, discriminator를 속이기 위한 학습이 아님)

  - GAN과 달리 generator에서 noised vector를 input으로 주지 않음

  - generator에서, random set의 random sampling 결과에 대한 backpropagation이 힘듦

- 즉, model input은

![fig](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbKdtEr%2FbtqEmoP86l1%2FVLg3H8POB0CsUz2gqb4v41%2Fimg.png)

- loss function은

![fig](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FoJT1L%2FbtqEoTHRhYu%2F77kHvsKbz4Kkkk16moQLMK%2Fimg.png)


<br>

# 3. Experiments

## 3.1 Experimental setup

![fig](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FqNNaO%2FbtqEoVsaOv5%2FauKRjaq8awq5IfteMwvsY0%2Fimg.png)

- GLUE task, SQuAD 수행하여 비교



## 3.2 model extensions

![fig](https://haebinshin.github.io/public/img/electra/figure3.png)

- 보다 효율적인 학습을 위해 아래 세가지 적용

  - `smaller generators`

  - `weight sharing`; 적은 학습으로도 성능을 낼 수 있도록 generator와 discriminator가 weight를 tie(공유함)

      - 그러나 generator는 작을수록 유리하기 때문에, input& output token embedding을 tied

  - `training algotithm`

      - 다양한 방법론 중에서, Electra의 generator와 discriminator를 jointly training 하는 방법이 제일 성능이 좋았음

## 3.3 small models

## 3.4. large models

## 3.5 Efficiency Analysis

![fig](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fk8UeF%2FbtqEm20khAp%2FeZ2Ae2dems8Jp2St9VQhv1%2Fimg.png)

- all token에 대해 loss를 계산하는 것이 효과적이다. 

<br>

# 4. Related Work


## Self-supervised Pre-training for NLP

- BERT, XLNET, tinyBERT, ...

## Generative Adversarial Networks

- GAN, MaskGAN, ...

## Contrastive Learning

- NCE(Noise-Contrastive Estimation), Word2Vec, CBOW, ...

<br>

# 5. Conclusion

- 본 논문은 language representation learning에 새로운 self-supervised task 를 제시했음

- key idea는, generator network로부터 생성되는 token이 negative sample인지 input token인지를 학습시키는 것이다.

- more compute-efficient, better performance

<br>

# Reference

- https://littlefoxdiary.tistory.com/41

<br>
