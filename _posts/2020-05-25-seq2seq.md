---
layout: post
title:  "[RNN]Seq2Seq : Neural Machine Translation"
date:   2020-05-25
desc: "seq2seq for machine translation"
keywords: "seq2seq, RNN, NMT, encoder, RBMT, embedding"
categories: [deep Learning]
tags: [seq2seq, RNN, NMT, encoder, RBMT, embedding]
icon: icon-html
---

<br>

`seq2seq` 이란 **입력된 시퀀스로부터 다른 도메인의 시퀀스를 출력하는 다양한 분야에서 사용되는 모델** 이다.

machine translation, chatbot, summarization, Speech Recognition, image captioning 등등 다양한 자연어 처리에서 쓰인다.


<br>


`encoder`, `decoder`, `generator`로 이루어져 있는 일종의 **autoencoder** 로,


[수식1]("static/assets/img/blog/RNN/argmax.png")에서의 **P(Y|X;Θ)** 를 최대로 하는 모델 파라미터를 찾도록 학습한 후


[수식2]('static/assets/img/blog/RNN/argmax2.png')와 같이, 사후 확률을 최대로 하는 Y를 찾아낸다.



<br>

#### Encoder

- 여러 개의 vector(주어진 소스 문장)를 입력으로 받아, 문장을 함축하는 문장 embedding vector로 만들어냄
  - 즉, P(z|X)를 모델링하고, 주어 문장을 Manifold를 따라 차원축소하여, 해당 domain의 latent space의 하나의 점에 투영함.

- machine translation을 위한 문장 embedding vector를 만들기 위해서는 feature를 최대한 많이 간직해야 함
  - 이것이 다른 text classification과 다른 점


- 수식으로 나타내기

[수식3]('static/assets/img/blog/RNN/seq2seq.png')

  - [;]는 concatenate 를 의미

<br>


#### Decoder

- 



<br>


#### Generator



<br>

Reference
---------
- [딥러닝을 이용한 자연어처리 입문](https://wikidocs.net/24996)
- [자연어처리 딥러닝 캠프](https://www.hanbit.co.kr/store/books/look.php?p_code=B1294694476)
